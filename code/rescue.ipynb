{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOsVdx6GGHmU"
   },
   "source": [
    "# RESCUE\n",
    "**Renewable Energy Salient Combined Uncertainty Estimator**\n",
    "\n",
    "## *Description:*\n",
    "A machine-learning based framework to quantify the short-term uncertainty in netload forecast developed by E3. \n",
    "The main strucutre of the model include a two layer artificial neural net with the pinball loss function as objective. Conditional on combinations of input, the model should be able to output quantile forecast for the net-load forecast error.\n",
    "\n",
    "This notebook contains the work load of ingesting pre-processed data, set up cross validation folds, training and deployment, and calling functions for diagnostics. For detailed implementation of data preprocessing or quoted functions, please refer to other script files. This project is available in the [e3/RESCUE](https://github.com/e3-/RESCUE) Github online repository. \n",
    "\n",
    "In the preliminary use case, the quantile forecast is trained on the response variable. For CAISO, as we are using RTPD forecast - RTD forecast as the response variable, the quantiles is actually on forecast difference rather than forecast error. Nevertheless, the model structure and the logic still holds the same. The code supports multi-output, multi-objective learning. For example, in addition to producing the quantile forecast of Net Load (RTPD-RTD) forecast error, the model can be trained to simultaneously trained to predict the Load (RTPD-RTD), Solar (RTPD-RTD) and Wind (RTPD-RTD) forecast error which would otherwise be embedded in the Net Load. The objectives can be weighted per user's judgement of relative importance.\n",
    "\n",
    "## *Highlights:*\n",
    "1. Incorporating a wide gamut of information: weather, calendar, forecast, and lagged error aware. \n",
    "2. Inherrent handles resource correlation as solar,wind, and load errors are co-trained within the model.\n",
    "3. Produces multiple prediction intervals for expected error in netload, load, solar and wind forecasting, for cherry picking down-stream\n",
    "4. Model agnostic. No requirement on knowledge of the inner workings of the netload forecast\n",
    "5. Adheres to best practice in statistics: cross validation, normalization, early-stopping, etc.\n",
    "\n",
    "## *To-dos:*\n",
    "1. Standardize the output for stability;\n",
    "2. Loop in Charlie's section on metrics\n",
    "3. Make actual reserves data part of dir_str if we foresee comparing model predictions to actual reserves in future work\n",
    "4. Delete any cells at the end that were used as scratch space\n",
    "\n",
    "## *Authors:* \n",
    "Yuchi Sun, Vignesh Venugopal, Charles Gulian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0trJmd6DjqBZ"
   },
   "outputs": [],
   "source": [
    "# Import third party packages\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Import self defined packages\n",
    "import cross_val\n",
    "import utility\n",
    "import diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. User Inputs\n",
    "## 0.1 Model Training and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the model to either be trained from scratch or to be restored for diagnostics. Recommend to include version number\n",
    "model_name = 'rescue_v1_1_multi_objective'\n",
    "\n",
    "# Target quantiles of the prediction \n",
    "PI_percentiles = np.array([0.025, 0.05, 0.25, 0.5,0.75, 0.95, 0.975]) # quantiles to predict\n",
    "\n",
    "# Structural parameter of the ANN network\n",
    "num_neurons = 10\n",
    "activation_type = 'relu'\n",
    "# This determines relative importance of our outputs. Order MUST match order of output variables in trainval_outputs data\n",
    "loss_wts = [np.sqrt(3), 1.0, 1.0, 1.0]\n",
    "num_outputs = len(loss_wts)\n",
    "\n",
    "# Cross validation parameters\n",
    "num_cv_folds = 10 # number of cross validation folds\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64 # size of each mini-batch in the SGD\n",
    "max_epochs = 50 # Maximum number of epochs in training. In each epoch, each training data is used exactly once\n",
    "optimizer_choice = 'adam' # Optimizer choice. Default to ADAM, a popular choice that have 1st and 2nd order momentum\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stop_monitor = 'val_loss' # The metrics to watch when deciding whether to stop. Recommendation: Validation loss as in 'val_loss'\n",
    "early_stop_min_delta = 0.5 # if the difference/decrease in loss is less than min_delta, the model is considered no longer improving\n",
    "early_stop_patience = 3 # For number of patience epochs, observe if the model has improved more than min_delta\n",
    "early_stop_verbosity = 1 # 0: no output, 1: some output, 2: full output\n",
    "\n",
    "# Check points parameters\n",
    "ckpt_monitor = 'val_loss' # Check points are only saved when there is an improvement in ckpt monitor\n",
    "\n",
    "# Losses and Metrics log parameters\n",
    "log_activation_freq = 0 # the frequency of logging hidden layer activation's histogram. Default to not log\n",
    "log_update_freq = 'epoch' # The frequency of logging. Default to record at the end of every epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Diagnostics and Visualizations Associated with Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolution of plots to be made\n",
    "default_dpi = 300\n",
    "\n",
    "# Define the mapping between internal feature (model input) name and feature label used in plotting\n",
    "label_to_feature_map = {\"Solar_RTPD_Forecast_T+1\":\"Solar Generation (MW)\",\n",
    "                       \"Wind_RTPD_Forecast_T+1\":\"Wind Generation (MW)\",\n",
    "                       \"Load_RTPD_Forecast_T+1\":\"Load (MW)\",\n",
    "                       \"Days_from_Start_Date_T+1\":\"Date of Observation\",\n",
    "                       \"Hour_Angle_T+1\":\"Hour\",\n",
    "                       \"Day_Angle_T+1\":\"Month\"}\n",
    "\n",
    "# Define the mapping between internal response (model output) name and response label used in plotting\n",
    "label_to_response_map = {\"Net_Load_Forecast_Error_T+1\":\"Net Load\",\n",
    "                       \"Load_Forecast_Error_T+1\":\"Load\",\n",
    "                       \"Solar_Forecast_Error_T+1\":\"Solar\",\n",
    "                       \"Wind_Forecast_Error_T+1\":\"Wind\"}\n",
    "\n",
    "# Define the mapping between internal response (model output) name and its type\n",
    "# Type MUST be one of \"load\" or \"generation\". Used to determine sign-convention while plotting\n",
    "response_type_map = {\"Net_Load_Forecast_Error_T+1\":\"load\",\n",
    "                       \"Load_Forecast_Error_T+1\":\"load\",\n",
    "                       \"Solar_Forecast_Error_T+1\":\"generation\",\n",
    "                       \"Wind_Forecast_Error_T+1\":\"generation\"}\n",
    "\n",
    "# Training and validation loss comparison for multiple folds\n",
    "x_jitter = 0.1\n",
    "metrics_to_idx_map = {'Loss (MW)':0, 'Coverage Probability (%)':1}\n",
    "metrics_to_compare = ['Loss (MW)', 'Coverage Probability (%)'] # choose in metrics_to_idx_map's keys\n",
    "\n",
    "# Time series\n",
    "ts_ranges = ['20170201','20170501','20170801','20171101'] # the example range of time series to plot\n",
    "\n",
    "# Do you wish to save diagnostic figures?\n",
    "save_fig_flag = True\n",
    "\n",
    "# Do you wish to see diagnostic figures for a specific month and/or year?\n",
    "# Leave with value \"All\" if you want the plots to be based on the entire dataset\n",
    "diagnostics_for_year = 2019\n",
    "diagnostics_for_month = 7\n",
    "\n",
    "# Define paths to historical FRP reserves data. Set thess variable to None if not applicable\n",
    "# Files MUST have an UP and a DOWN column, holding headroom and footroom in MW, both of which MUST be positive\n",
    "histogram_rtpd_reserves_path = \"C:\\\\Users\\\\vignesh.venugopal\\\\PycharmProjects\\\\RESCUE\\\\data\\\\raw_data\\\\Flex_Ramp_Req_RTPD_Histogram.csv\"\n",
    "quant_reg_rtpd_reserves_path = \"C:\\\\Users\\\\vignesh.venugopal\\\\PycharmProjects\\\\RESCUE\\\\data\\\\raw_data\\\\Flex_Ramp_Req_RTPD_Quantile_Reg.csv\"\n",
    "\n",
    "# We have 'k' models wherein k is the number of cv folds. Pick one of the models to load predictions\n",
    "# and calculate diagnostic metrics and create diagnostic plots for\n",
    "model_idx = 0\n",
    "\n",
    "# Do you wish to save the coincident model net load predictions and ground truth data being identified and \n",
    "# plotted in section 5.3?\n",
    "save_net_load_pred_truth_data_flag = True\n",
    "\n",
    "# Quantiles of true data to be plotted; a low, mid and high. Mid will most probably be 0.5 (i.e median) while\n",
    "# low and high will correspond to those that set footroom and headroom requirements in the system of interest\n",
    "# Since the true data will be compared to model predictions, ensure model predictions exist at these exact quantiles\n",
    "quantiles_of_true_data = [0.025, 0.5, 0.975]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NAbSZiaoJ4z"
   },
   "source": [
    "# 1. Data Ingestion\n",
    "\n",
    "Load in pre-processed trainval dataset that includes all input features/ output response(s) for both training and validation. Prepare the cross validation splitting masks. Set up directory structure to store intermediate (`log`, `checkpoints`) and final outputs (`models`, `outputs`,`diagnostics`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in/ Create folder structure for the current model\n",
    "dir_str = utility.Dir_Structure(model_name = model_name)\n",
    "\n",
    "# Read in input and output of the training and validation samples from data pipeline. \n",
    "# This should be an output of the data_preprocessing script\n",
    "input_trainval = pd.read_pickle(dir_str.input_trainval_path)\n",
    "output_trainval = pd.read_pickle(dir_str.output_trainval_path)\n",
    "assert input_trainval.shape[0] == output_trainval.shape[0], \"Input and output shape mismatch!\"\n",
    "if num_outputs > 1:\n",
    "    assert output_trainval.shape[1] == num_outputs, \"Number of 'loss_wts' must equal number of outputs\"\n",
    "n_samples = input_trainval.shape[0]\n",
    "\n",
    "# Use cross validation script to conduct intra-day consecutive trainval splitting. The number of folds is \n",
    "# determined by num_cv_folds. The data of the same day would not end up separately in training and validation\n",
    "# to not overestimate model performance.\n",
    "val_masks_all_folds = cross_val.get_CV_masks(input_trainval.index, num_cv_folds, dir_str.shuffled_indices_path)\n",
    "\n",
    "# confirm the PI percentiles are symmetrical and 0.5 is one of the target quantile\n",
    "for PI in PI_percentiles:\n",
    "    assert np.allclose(1-PI_percentiles, PI_percentiles[::-1]), \"Not all PI intervals are constructed symmetrically!\"\n",
    "assert 0.5 in PI_percentiles, \"Median forecast (P50) must be produced!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Definition\n",
    "Define the various component of the model that are active at various stage of the model life cycle. Before training: model structure, loss function. During training: call backs and metrics. After training: Saving functions. \n",
    "\n",
    "## 2.1 Model Structure.\n",
    "The RESCUE model is built with the Keras [functional API](https://www.tensorflow.org/guide/keras#model_subclassing). As it stands right now, it is a two layer ANN network with a pre-processing normalization layer. Rectified linear units is used as the activation function for the hidden layers, while the last layer is a direct linear regression.                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model that include the normalization layer\n",
    "inputs = tf.keras.Input(shape=input_trainval.shape[1:])\n",
    "\n",
    "# Create a Normalization layer and set its internal state using the training data\n",
    "normalizer = tf.keras.layers.experimental.preprocessing.Normalization(name = 'Normalization')\n",
    "norm_inputs = normalizer(inputs)\n",
    "\n",
    "# A two-layer ANN network for regression\n",
    "dense1 = tf.keras.layers.Dense(num_neurons, activation=activation_type)\n",
    "hidden1 = dense1(norm_inputs)\n",
    "dense2 = tf.keras.layers.Dense(num_neurons, activation=activation_type)\n",
    "hidden2 = dense2(hidden1)\n",
    "dense3 = tf.keras.layers.Dense(num_outputs)\n",
    "outputs = dense3(hidden2)\n",
    "\n",
    "# define model from inputs to outputs\n",
    "rescue_model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Loss Function\n",
    "\n",
    "The pinball loss function is used here to provide a quantile forecast rather than a median forecast. With increasing sample size, the pinball loss would converge to the quantile forecast of a variable conditional on the input variable. The quantile is given by a parameter `tau`, which is set in the user input as PI percentiles. \n",
    "\n",
    "One property of the pinball loss is that for 0% and 100% percentile, the pinball loss is always 0 no matter the model and parameter choice, while the median forecast have the highest loss. So comparing the losses across different quantiles makes little sense, and is advised against. For more information on pinball losses, check out [this wiki](https://en.wikipedia.org/wiki/Quantile_regression).\n",
    "\n",
    "In case of multi-output, multi-objective regression, the loss corresponding to each output can be weighted by the user to differentiate the amt of importance he/she wishes to give to each output. The loss that the model will ultimately try to minimize would be the wt.avg loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PinballLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, tau=0.5, loss_wts=[1], name=\"pinball_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.tau = tau # the target quantile\n",
    "        self.loss_wts = loss_wts # Relative importance of loss corresponding to each output\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        err = y_true - y_pred # the convention is always true - pred\n",
    "        # essentially, quantile regression takes two region. For values bigger than the quantile forecast,\n",
    "        # they are weighted by 1-tau, while for values smaller than the forecast it's weighted by tau.\n",
    "        loss_for_each_output_var_of_each_sample = tf.math.maximum(self.tau * err, (self.tau - 1) * err)\n",
    "        # Get weighted avg loss for each sample\n",
    "        loss_for_each_sample = tf.math.reduce_mean(tf.math.multiply(loss_for_each_output_var_of_each_sample, self.loss_wts),\n",
    "                                                   axis = 1)\n",
    "        # Now get a single loss across all samples that make up the batch\n",
    "        skewed_mse = tf.math.reduce_mean(loss_for_each_sample, axis=0)\n",
    "\n",
    "        return skewed_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Metrics\n",
    "\n",
    "In tensorflow terminology, metrics are quantities that are calculated as the training goes on to aid your judgement on model's fitness and completeness. In our use cases, we define two metrics for this purpose: coverage probability and average interval width.\n",
    "\n",
    "For prediction intervals, coverage probability refers to how often are the actual forecast included in the prediciton interval bands. Here we slightly modify the definition and refers to how often are the actual forecast smaller than the target quantile. For a well behaving model, the CP would converge to target quantile tau.\n",
    "\n",
    "For average interval width, it normally refers to the width of a prediction interval band. Again, we make a slight modification here. Since in practice, the forecast for quantiles above 50% are upwards reserve and normally positive, and below 50% are downwards reserve and normally negative, we are simply using the quantile forecast's absolute distance to 0 as the interval width here. For interpretation, we are looking for smaller requirement for better band width, but also high flexibitliy wrt to varying condidtions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoverageProbability(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name = 'CP',**kwargs):\n",
    "        super(CoverageProbability, self).__init__(name = name, **kwargs)\n",
    "        self.coverage_probability = self.add_weight(name = 'CP', initializer=\"zeros\", dtype = tf.float64)\n",
    "        # the cumulative number of samples and the number of samples smaller than current quantile forecast\n",
    "        self.cum_n_samples = self.add_weight(name = 'n_samples', initializer=\"zeros\", dtype = tf.int32)\n",
    "        self.cum_n_covered = self.add_weight(name = 'n_covered', initializer=\"zeros\", dtype = tf.int32)\n",
    "        \n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # the state would be updated everytime we have a new calculation\n",
    "        self.cum_n_samples.assign_add(tf.size(y_pred, out_type=tf.int32))\n",
    "        self.cum_n_covered.assign_add(tf.math.count_nonzero(tf.math.less_equal(y_true,y_pred), dtype = tf.int32))\n",
    "        # cp = n_covered/n_samples\n",
    "        self.coverage_probability.assign(tf.math.divide(self.cum_n_covered, self.cum_n_samples))\n",
    "\n",
    "    def result(self):\n",
    "        return self.coverage_probability\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.coverage_probability.assign(0.0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Callbacks\n",
    "\n",
    "In tensorflow terminology, callbacks are functions that get executed with certain frequency during training. For our purpose, all the callbacks happen once per epoch and we are using three types of callbacks: Early stopping, check points, and tensor boards. Saving the model is not necessarily a callback, but it also get executed once after the training for the model is complete, so is included in this segment.\n",
    "\n",
    "1. Early stopping stops the training when certain criteria is met. In general the criteria is that when `monitor` did not improve by more than `min_delta` in `patience` epoch(s), then the training is stopped. This is a way to effectively prevent overfitting to the training data, so the monitor is most often validation loss.  \n",
    "2. Check points are periodical snap shot of parameter weights saved in case the model training is unexpectedly stopped.  We are saving the `checkpoints`,`logs`, and`trained models` in different folders for different taus and folds.\n",
    "3. Saving is very similar to check points. Difference is that check points are conducted at the end of every epoch, while saving only happens at the end of training session.\n",
    "4. The tensorboard callback allow us to visualize and observe losses and all metrics in a pre-compiled tensorboard interface. It can even visualize losses and metrics for multiple folds and taus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping stops the training when certain criteria is met.\n",
    "cb_early_stopping = tf.keras.callbacks.EarlyStopping(monitor=early_stop_monitor, min_delta=early_stop_min_delta, \n",
    "                                                     patience=early_stop_patience, verbose=early_stop_verbosity)\n",
    "\n",
    "# For the save best only parameter: we will overwrite the current checkpoint if and only if the `val_loss` \n",
    "# score has improved. Different fold and tau would end up in different ckpts_dir folder\n",
    "def get_cb_check_points(tau, fold_idx):\n",
    "    # make sure models for different tau go to different directories\n",
    "    ckpts_dir = os.path.join(dir_str.ckpts_dir, \"tau_{:.1%}\".format(tau),\"fold_#{}\".format(fold_idx))\n",
    "    if not os.path.exists(ckpts_dir):\n",
    "        os.makedirs(ckpts_dir)\n",
    "    cb_check_points = tf.keras.callbacks.ModelCheckpoint(filepath=ckpts_dir, save_best_only=True, monitor= ckpt_monitor, verbose=0)\n",
    "    return cb_check_points\n",
    "\n",
    "\n",
    "# Currrently not logging the histogram of activation and embedding layers. Write log per epoch.\n",
    "def get_cb_tensor_board(tau, fold_idx):\n",
    "    # make sure models for different tau and fold would get logged in different directory\n",
    "    logs_dir = os.path.join(dir_str.logs_dir, \"tau_{:.1%}\".format(tau),\"fold_#{}\".format(fold_idx))\n",
    "    if not os.path.exists(logs_dir):\n",
    "        os.makedirs(logs_dir)\n",
    "    cb_tensor_board = tf.keras.callbacks.TensorBoard(logs_dir, histogram_freq= log_activation_freq, \n",
    "                                                     embeddings_freq=0,  update_freq=log_update_freq)  \n",
    "    return cb_tensor_board\n",
    "\n",
    "# Save the model by the end of each training session. Might be replacible by checkpoints.\n",
    "def save_rescue_model(model, tau, fold_idx):\n",
    "    # make sure models for different tau and fold would get logged in different directory\n",
    "    models_dir = os.path.join(dir_str.models_dir, \"tau_{:.1%}\".format(tau),\"fold_#{}\".format(fold_idx))\n",
    "    if not os.path.exists(models_dir):\n",
    "        os.makedirs(models_dir)\n",
    "    \n",
    "    model.save(models_dir)\n",
    "    return None   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Training\n",
    "\n",
    "Training is the process where the parameter of a model changes to reduce some loss function. In our specific case, we are conducting training separtely for each target quantile and each fold. We first split data into training and validation based on current fold number, and then initialize a new model to fit to the training data until the pinball loss meet some stopping criteria, i.e. showing no significant decrease.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescue_model_set = {} # intialized container for TF models. Indexed by (tau, fold_idx)\n",
    "history = {} # intialized container for the training history of models. Indexed by (tau, fold_idx)\n",
    "\n",
    "# loop through different target quantiles and cross validation folds\n",
    "for tau in PI_percentiles:\n",
    "    print(\"Training model for Prediction interval: {:.1%}\".format(tau))\n",
    "    for fold_idx in range(num_cv_folds):\n",
    "        print(\"Cross Validation fold #\", fold_idx+1)\n",
    "        \n",
    "        # Split into training and validation dataset based on the CV validation masks generated in section 1.\n",
    "        input_train, output_train = input_trainval[~val_masks_all_folds[fold_idx]], output_trainval[~val_masks_all_folds[fold_idx]]\n",
    "        input_val, output_val = input_trainval[val_masks_all_folds[fold_idx]], output_trainval[val_masks_all_folds[fold_idx]]\n",
    "        \n",
    "        # retain value only and cast to 'float32'. Single precision calculate a lot faster than double precision.\n",
    "        input_train = input_train.values.astype('float32')\n",
    "        output_train = output_train.values.astype('float32')\n",
    "        input_val = input_val.values.astype('float32')\n",
    "        output_val = output_val.values.astype('float32')\n",
    "\n",
    "        # Using tf.data API to batch and shuffle the dataset. For shuffling, the buffer size should be bigger than the total \n",
    "        # sample count. Or else only the first buffle size of samples would be shuffled \n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((input_train, output_train)).shuffle(buffer_size= n_samples).batch(batch_size)\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((input_val, output_val)).shuffle(buffer_size = n_samples).batch(batch_size)\n",
    "\n",
    "        # Make a fresh clone of the rescue model for the specific quantile and fold\n",
    "        rescue_model_set[(tau, fold_idx)] = tf.keras.models.clone_model(rescue_model)\n",
    "        # For some layers the paramers are not trainable, and is adapted at the begining to data. E.g.:Normalization layer \n",
    "        rescue_model_set[(tau, fold_idx)].get_layer('Normalization').adapt(input_trainval.values)\n",
    "        # Compiling the loss, optimizer, metrics, and model into one compiled instance\n",
    "        rescue_model_set[(tau, fold_idx)].compile(loss=PinballLoss(tau=tau, loss_wts=loss_wts),\n",
    "                                                  optimizer=optimizer_choice, metrics=[CoverageProbability()])\n",
    "        \n",
    "        # The training process. Passing in callbacks to use in mid training \n",
    "        history[(tau, fold_idx)]= rescue_model_set[(tau, fold_idx)].fit(train_ds, validation_data=val_ds, epochs=max_epochs,\n",
    "                                                                        callbacks=[cb_early_stopping, get_cb_tensor_board(tau, fold_idx)])\n",
    "        \n",
    "        # Save the trained rescue model for each target percentile and fold\n",
    "        save_rescue_model(rescue_model_set[(tau, fold_idx)], tau, fold_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Inferrence\n",
    "After the model is trained, we use it to produce quantile predictions on the entire trainval set. Note that the training history is also transformed into a np array here for easy storage and visualizaiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize container for the inference result for different quantiles/CV folds\n",
    "multi_index_tau_folds = pd.MultiIndex.from_product([PI_percentiles, range(num_cv_folds), output_trainval.columns.values],\n",
    "                                                   names = ['Quantiles','Fold ID', \"Output_Name\"])\n",
    "pred_trainval = pd.DataFrame(index = input_trainval.index, columns = multi_index_tau_folds) \n",
    "\n",
    "# Initialize container for the training loss and metric history\n",
    "num_metrics = len(history[(tau, fold_idx)].params['metrics'])\n",
    "training_history = np.ones((len(PI_percentiles), num_cv_folds, max_epochs, num_metrics))*np.nan\n",
    "\n",
    "# looping through all target percentiles and CV folds\n",
    "for i,tau in enumerate(PI_percentiles):\n",
    "    print ('Inferring on quantile of {:.1%}'.format(tau))\n",
    "    for fold_idx in range(num_cv_folds):\n",
    "        # Record training loss and metrics history\n",
    "        num_epochs = len(history[(tau, fold_idx)].epoch)\n",
    "        training_history[i,fold_idx,:num_epochs,:] = pd.DataFrame(history[(tau, fold_idx)].history).values\n",
    "        \n",
    "        # Deploy model on the trainval data and record inference results\n",
    "        pred_trainval.loc[:, (tau, fold_idx)] = rescue_model_set[(tau,fold_idx)].predict(input_trainval.values)\n",
    "        \n",
    "#Output inferrence result and training history to hard drive        \n",
    "pred_trainval.to_pickle(dir_str.pred_trainval_path)\n",
    "np.save(dir_str.training_hist_path, training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Diagnostics and Visualizations\n",
    "This section includes an expanding set of diagnostics and visualization tool to assess the performance of RESCUE model, and check whether the model's behavior adheres to our intuition. As a standalone section, you should be able to skip **section 2-4** and directly start runing **section 5** after **section 1**. First we load in the quantile prediction made in the inferrence section and the training history from the training section. They should all be on the hard drive already if previously trained to completion and saved. Then, we optionally separate out specific month/year of the quantile predictions and read in actual historical reserves' data for comparison, if the user has provided associated inputs in **section 0.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in quantile prediction\n",
    "if os.path.exists(dir_str.pred_trainval_path):\n",
    "    pred_trainval_all = pd.read_pickle(dir_str.pred_trainval_path)\n",
    "    pred_trainval = pred_trainval_all.xs(key = model_idx, axis = 1, level = 'Fold ID')\n",
    "else:\n",
    "    print('No quantile prediction found! Run section 4 first!')\n",
    "# load in training history\n",
    "if os.path.exists(dir_str.training_hist_path):\n",
    "    training_hist = np.load(dir_str.training_hist_path)\n",
    "else:\n",
    "    print('No training history found!')\n",
    "    \n",
    "# If the user has chose to view these diagnostics for a particular month and/or year, extract the relevant time-period\n",
    "# from the larger dataset\n",
    "pred_trainval_diag = pred_trainval.copy()\n",
    "input_trainval_diag = input_trainval.copy()\n",
    "output_trainval_diag = output_trainval.copy()\n",
    "if diagnostics_for_month is not \"All\":\n",
    "    pred_trainval_diag = pred_trainval_diag.loc[pred_trainval_diag.index.month == diagnostics_for_month, :]\n",
    "    input_trainval_diag = input_trainval_diag.loc[input_trainval_diag.index.month == diagnostics_for_month, :]\n",
    "    output_trainval_diag = output_trainval_diag.loc[output_trainval_diag.index.month == diagnostics_for_month, :]\n",
    "if diagnostics_for_year is not \"All\":\n",
    "    pred_trainval_diag = pred_trainval_diag.loc[pred_trainval_diag.index.year == diagnostics_for_year, :]\n",
    "    input_trainval_diag = input_trainval_diag.loc[input_trainval_diag.index.year == diagnostics_for_year, :]\n",
    "    output_trainval_diag = output_trainval_diag.loc[output_trainval_diag.index.year == diagnostics_for_year, :]\n",
    "\n",
    "# Read in rtpd reserves data, if user provided a path to it\n",
    "if histogram_rtpd_reserves_path is None:\n",
    "    histogram_rtpd_reserves = None\n",
    "else:\n",
    "    # Read in historical rtpd reserves to overlay on net-load graphs\n",
    "    histogram_rtpd_reserves = pd.read_csv(histogram_rtpd_reserves_path, index_col = 0, parse_dates = True)\n",
    "    # Extract appropriate time-points from historical reserves data\n",
    "    histogram_rtpd_reserves = histogram_rtpd_reserves.loc[input_trainval_diag.index.values, :]\n",
    "if quant_reg_rtpd_reserves_path is None:\n",
    "    quant_reg_rtpd_reserves = None\n",
    "else:\n",
    "    # Read in historical rtpd reserves to overlay on net-load graphs\n",
    "    quant_reg_rtpd_reserves = pd.read_csv(quant_reg_rtpd_reserves_path, index_col = 0, parse_dates = True)\n",
    "    # Extract appropriate time-points from historical reserves data\n",
    "    quant_reg_rtpd_reserves = quant_reg_rtpd_reserves.loc[input_trainval_diag.index.values, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Tensorboard\n",
    "Tensorboard is a built-in visualization tool to visualize the losses and metrics during multiple training processes. Use `tensorboard --logdir dir_str.logs_dir` to invoke it in the command line (replace `dir_str.logs_dir` with the folder it's referring to). This is also the reason we did not plot training history for each epoch, as it's done in tensorboard already\n",
    "\n",
    "**What to look for**: Training loss should converge in the last few epochs. Validation loss should not trail behind training loss by too much.\n",
    "\n",
    "\n",
    "## 5.2 Forecast uncertainty grouped by input features\n",
    "Visualize the forecast bias and uncertainty's change w.r.t to different input features. Check if the visualization corroborate or contradict our intuition. Many contradictions would constitute a red flag for model's fidelity. The sign convention is managed such that headroom requirement will always be shown above y = 0 and footroom will always be shown below y = 0, irrespective of whether you are looking at load-based or a generation-based forecast uncertainty.\n",
    "\n",
    "**What to expect with net load forecast:** <br>\n",
    "1. **Forecast uncertainty v.s. Solar production**: Forecast uncertainty should be the highest for medium solar production, and low during both no solar and full solar periods.\n",
    "2. **Forecast uncertainty v.s. Wind production**:  Wind should have less effect on forecast uncertainty than solar. Increases with higher wind production\n",
    "3. **Forecast uncertainty v.s. Load**: Forecast uncertainty should increase with higher load.\n",
    "4. **Forecast uncertainty v.s. Hours**: Sunset and sunrise hours are associated with high amount of uncertainty. No solar night time hours should have low uncertainty.\n",
    "5. **Forecast uncertainty v.s. Season**: Highly dependent on local climate type. For mediterranean climate, expect little to no change, with a slight increase in winter. For continental climate, expect higher uncertainty in summer due to precipitation. For desert climate, expect smaller uncertainty year round, while expecting higher uncertainty year round for oceanic and tropical climates. For monsoon dominated climate, expect higher uncertainty in monsoon season (usually summer).\n",
    "6. **Forecast uncertainty v.s. Date of installation**: Highly dependent on modeled BA. If the BA experience significant renewable growth, expect increasing uncertainty with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Demonstrate the change in forecast uncertainty with respect to Solar, wind and load\n",
    "for feature, label in label_to_feature_map.items():\n",
    "    # if looking at Hour and Month, binning is derived from index\n",
    "    if label == \"Hour\":\n",
    "        feature_discretized = pred_trainval_diag.index.hour\n",
    "    elif label == 'Month':\n",
    "        feature_discretized = pred_trainval_diag.index.month\n",
    "    else:\n",
    "        # else the label is derived from dicretizing a continuous input\n",
    "        feature_discretized = diagnostics.discretize_input(input_trainval_diag[feature])\n",
    "        if label == 'Date of Observation':\n",
    "            # for 'Date of Observation', we add the starting date so it shows up as a historical data\n",
    "            feature_discretized = (pd.Timestamp(input_trainval.index[0].date()) \n",
    "                                   + feature_discretized.astype('int')*pd.Timedelta('1D'))\n",
    "\n",
    "    # Plotting the uncertainty (of each model output) and bias for each feature bin\n",
    "    for response, response_label in label_to_response_map.items():\n",
    "        pred_trainval_for_current_response = pred_trainval_diag.xs(key = response, axis = 1, level = 'Output_Name')\n",
    "        # Feed in actual RTPD reserves to overlay on net load graphs\n",
    "        if response_label == \"Net Load\":\n",
    "            histogram_rtpd_res_data = histogram_rtpd_reserves\n",
    "            quant_reg_rtpd_res_data = quant_reg_rtpd_reserves\n",
    "        else:\n",
    "            histogram_rtpd_res_data = None\n",
    "            quant_reg_rtpd_res_data = None\n",
    "        # Feed in actual inputs and outputs for current selection of feature and response to be overlaid on top of\n",
    "        # graph showing forecast error uncertainty\n",
    "        trainval_inputs_current_feature = input_trainval_diag[feature]\n",
    "        trainval_outputs_current_response = output_trainval_diag[response]  \n",
    "        # Feed in response type to determine sign convention while plotting\n",
    "        response_type = response_type_map[response]\n",
    "        # Make plot with chosen specifications       \n",
    "        fig, axarr = diagnostics.plot_uncertainty_groupedby_feature(pred_trainval_for_current_response, feature_discretized, \n",
    "                                                                label_to_feature_map[feature], response_label,\n",
    "                                                                response_type,\n",
    "                                                                trainval_inputs_current_feature,\n",
    "                                                                trainval_outputs_current_response,\n",
    "                                                                histogram_rtpd_res_data,\n",
    "                                                                quant_reg_rtpd_res_data,\n",
    "                                                                quantiles_of_true_data)\n",
    "    \n",
    "        # Special formatting of the x axis when we are using date observation\n",
    "        if label == 'Date of Observation':\n",
    "            fig.autofmt_xdate()\n",
    "\n",
    "        # Save fig if user so desires\n",
    "        if save_fig_flag:\n",
    "            fig_file_name = \"{}_vs_{}_year_{}_mth_{}.svg\".format(response_label, label, \n",
    "                                                                 str(diagnostics_for_year), str(diagnostics_for_month))\n",
    "            fig.savefig(os.path.join(dir_str.plots_dir, fig_file_name), dpi = default_dpi)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Comparison of Predictions to Truth\n",
    "Visualize and compare the model predictions to the ground truth for points that lie at the very edge of the desired prediction interval in each hour. True reserves held can be overlaid on top for comparison with model predicted reserves. The sign convention is managed such that headroom requirement will always be shown above y = 0 and footroom will always be shown below y = 0, irrespective of whether you are looking at load-based or a generation-based forecast uncertainty.\n",
    "\n",
    "**What to expect from a well trained model:** <br>\n",
    "1. True forecast errors are generally comprised within the extent of model predictions\n",
    "2. Model's predictions doesn't fall well beyond true forecast errors all the time - which would be an indication that the model is being too conservative and thus wasteful of reserves. We're aiming to achieve target coverage while minimizing amount of reserves held to do so\n",
    "3. Median quantile of true forecast errors matches well with median quantile of model predictions of forecast errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to compare model predictions to truth at critical quantiles in each hour\n",
    "feature = \"Hour_Angle_T+1\"\n",
    "label = \"Hour\"\n",
    "# Plotting each model output \n",
    "for response, response_label in label_to_response_map.items():\n",
    "    pred_trainval_for_current_response = pred_trainval_diag.xs(key = response, axis = 1, level = 'Output_Name')\n",
    "    # Feed in actual RTPD reserves to overlay on net load graphs\n",
    "    if response_label == \"Net Load\":\n",
    "        histogram_rtpd_res_data = histogram_rtpd_reserves\n",
    "        quant_reg_rtpd_res_data = quant_reg_rtpd_reserves\n",
    "    else:\n",
    "        histogram_rtpd_res_data = None\n",
    "        quant_reg_rtpd_res_data = None\n",
    "    # Feed in ground truth for current selection of response to be overlaid on top of graph showing model output\n",
    "    trainval_outputs_current_response = output_trainval_diag[response]  \n",
    "    # Feed in response type to determine sign convention while plotting\n",
    "    response_type = response_type_map[response]\n",
    "    # Make plot with these specifications\n",
    "    fig, ax, truth_df, pred_df = diagnostics.compare_predictions_to_truth(pred_trainval_for_current_response, \n",
    "                                                       label_to_feature_map[feature], response_label,\n",
    "                                                       response_type,\n",
    "                                                       trainval_outputs_current_response,\n",
    "                                                       histogram_rtpd_res_data,\n",
    "                                                       quant_reg_rtpd_res_data,\n",
    "                                                       quantiles_of_true_data)\n",
    "    # Save fig is user so desires\n",
    "    if save_fig_flag:\n",
    "        fig_file_name = response_label + \"_compared_to_truth_in_month_{}_of_year_{}.svg\".format(diagnostics_for_month,\n",
    "                                                                                          diagnostics_for_year)\n",
    "        fig.savefig(os.path.join(dir_str.plots_dir, fig_file_name), \n",
    "                    dpi = default_dpi,\n",
    "                    bbox_inches = \"tight\")\n",
    "    \n",
    "    # Save the net load data being plotted if the user so desires\n",
    "    if response_label == \"Net Load\":\n",
    "        if save_net_load_pred_truth_data_flag:\n",
    "            truth_file_name = \"True_\" + response_label + \"_error_in_month_{}_of_year_{}.csv\".format(diagnostics_for_month,\n",
    "                                                                                          diagnostics_for_year)\n",
    "            truth_df.to_csv(os.path.join(dir_str.diag_dir, truth_file_name))\n",
    "            pred_file_name = \"Predicted_\" + response_label + \"_error_in_month_{}_of_year_{}.csv\".format(diagnostics_for_month,\n",
    "                                                                                          diagnostics_for_year)\n",
    "            pred_df.to_csv(os.path.join(dir_str.diag_dir, pred_file_name))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Model performance in different CV folds\n",
    "Visualizing model performance for all the cross validation folds across training and validation sets. An important tool in assessing the generalizability of the model.\n",
    "\n",
    "**What to look for:**\n",
    "1. Distance between training and validation. Smaller distance signifies a small amount of expected performance drop betweeen seen and unseen data, hence better generalizability.\n",
    "2. Spread among different validation folds: The smaller the spread, the more invariant the model is to its data, and hence better generalizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, _ =  diagnostics.plot_compare_train_val(training_hist, PI_percentiles,      \n",
    "                                             metrics_to_idx_map, metrics_to_compare, x_jitter =x_jitter)\n",
    "if save_fig_flag:\n",
    "    fig.savefig(os.path.join(dir_str.plots_dir, 'Train_Val_Comparison.png'), dpi=default_dpi, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Example Time series\n",
    "Visualize the quantile forecast for a few example days. The sign convention is managed such that headroom requirement will always be shown above y = 0 and footroom will always be shown below y = 0, irrespective of whether you are looking at load-based or a generation-based forecast uncertainty.\n",
    "\n",
    "\n",
    "**What to look for**:\n",
    "1. **Smoothness**: Is the reserve interval band changing continuously?\n",
    "2. **Coverage**: How good is the reserve interval band covering the true forecast error? The more the better.\n",
    "3. **Exceeding**: What kind of condition seems to lead to exceedance? Hopefully it should be extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for response, response_label in label_to_response_map.items():\n",
    "    pred_trainval_for_current_response = pred_trainval.xs(key = response, axis = 1, level = 'Output_Name')\n",
    "    output_trainval_for_current_response = output_trainval[response]\n",
    "    # Feed in response type to determine sign convention while plotting\n",
    "    response_type = response_type_map[response]\n",
    "    fig, axarr = diagnostics.plot_example_ts(ts_ranges, pred_trainval_for_current_response, \n",
    "                                             output_trainval_for_current_response, response_label, response_type)\n",
    "    if save_fig_flag:\n",
    "        fig_file_name = \"{}_on_chosen_specific_days.svg\".format(response_label)\n",
    "        fig.savefig(os.path.join(dir_str.plots_dir,fig_file_name), \n",
    "                    dpi = default_dpi, bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ix4mEL65on-w"
   },
   "source": [
    "Archived code: Use `tf.GradientTape` for very fine grained control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIKdEzHAJGt7",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "# test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "\n",
    "# @tf.function\n",
    "# def train_step(inputs, outputs, model, loss_fn):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         # training=True is only needed if there are layers with different\n",
    "#         # behavior during training versus inference (e.g. Dropout).\n",
    "#         predictions = model(inputs, training=True)\n",
    "#         loss = loss_fn(outputs, predictions)\n",
    "#     gradients = tape.gradient(loss, rescue.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, rescue.trainable_variables))\n",
    "\n",
    "#     train_loss(loss)\n",
    "#     #train_accuracy(outputs, predictions)\n",
    "\n",
    "\n",
    "# @tf.function\n",
    "# def test_step(inputs, outputs, model, loss_fn):\n",
    "#     # training=False is only needed if there are layers with different\n",
    "#     # behavior during training versus inference (e.g. Dropout).\n",
    "#     predictions = rescue(inputs, training=False)\n",
    "#     t_loss = loss_object(outputs, predictions)\n",
    "\n",
    "#     test_loss(t_loss)\n",
    "#     #test_accuracy(outputs, predictions)\n",
    "\n",
    "# EPOCHS = 5\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#   # Reset the metrics at the start of the next epoch\n",
    "#   train_loss.reset_states()\n",
    "#   #train_accuracy.reset_states()\n",
    "#   test_loss.reset_states()\n",
    "#   #test_accuracy.reset_states()\n",
    "\n",
    "#   for inputs, outputs in train_ds:\n",
    "#     train_step(inputs, outputs)\n",
    "\n",
    "#   for test_inputs, test_outputs in test_ds:\n",
    "#     test_step(test_inputs, test_outputs)\n",
    "\n",
    "#   print(\n",
    "#     f'Epoch {epoch + 1}, '\n",
    "#     f'Loss: {train_loss.result()}, '\n",
    "#     #f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "#     f'Test Loss: {test_loss.result()}, '\n",
    "#     #f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "advanced.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
