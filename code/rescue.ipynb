{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOsVdx6GGHmU"
   },
   "source": [
    "# RESCUE\n",
    "**Renewable Energy Salient Combined Uncertainty Estimator**\n",
    "\n",
    "## *Description:*\n",
    "A machine-learning based framework to quantify the short-term uncertainty in netload forecast developed by E3. \n",
    "The main strucutre of the model include a two layer artificial neural net with the pinball loss function as objective. Conditional on combinations of input, the model should be able to output quantile forecast for the net-load forecast error.\n",
    "\n",
    "This notebook contains the work load of ingesting pre-processed data, set up cross validation folds, training and deployment, and calling functions for diagnostics. For detailed implementation of data preprocessing or quoted functions, please refer to other script files. This project is available in the [e3/RESCUE](https://github.com/e3-/RESCUE) Github online repository. \n",
    "\n",
    "In the preliminary use case, the quantile forecast is trained on the response variable. For CAISO, as we are using RTPD forecast - RTD forecast as the response variable, the quantiles is actually on forecast difference rather than forecast error. Nevertheless, the model structure and the logic still holds the same.\n",
    "\n",
    "## *Highlights:*\n",
    "1. Incorporating a wide gamut of information: weather, calendar, forecast, and lagged error aware. \n",
    "2. Inherrent handles resource correlation as solar,wind, and load errors are co-trained within the model.\n",
    "3. Produces multiple prediction intervals for expected error in netload forecasting, for cherry picking down-stream\n",
    "4. Model agnostic. No requirement on knowledge of the inner workings of the netload forecast\n",
    "5. Adheres to best practice in statistics: cross validation, normalization, early-stopping, etc.\n",
    "\n",
    "## *To-dos:*\n",
    "1. Standardize the output for stability;\n",
    "\n",
    "## *Authors:* \n",
    "Yuchi Sun, Vignesh Venugopal, Charles Gulian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0trJmd6DjqBZ"
   },
   "outputs": [],
   "source": [
    "# Import third party packages\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Import self defined packages\n",
    "import cross_val\n",
    "import utility\n",
    "import diagnostics\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing metrics for RESCUE and CAISO histogram in March, July 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESCUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outputs = pd.read_pickle(\"C:\\\\Users\\\\charles.gulian\\\\PycharmProjects\\\\RESCUE\\\\code\\\\data_for_heatmaps\\\\inputs_for_training\\\\output_trainval.pkl\")\n",
    "df_preds = pd.read_pickle(\"C:\\\\Users\\\\charles.gulian\\\\PycharmProjects\\\\RESCUE\\\\code\\\\data_for_heatmaps\\\\outputs_from_trained_models\\\\pred_trainval.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9948078920041536\n",
      "702.4178950176721\n",
      "672.6594839441271\n",
      "95.08301361762179\n"
     ]
    }
   ],
   "source": [
    "month = 7\n",
    "\n",
    "y_true = df_outputs.loc[(df_outputs.index.year == 2019) & (df_outputs.index.month == month), 'Net_Load_Forecast_Error_T+1']\n",
    "y_pred = df_preds[(0.975, 2, 'Net_Load_Forecast_Error_T+1')].loc[(df_preds.index.year == 2019) & (df_preds.index.month == month)]\n",
    "\n",
    "for metric in [coverage, requirement, closeness, exceeding]:\n",
    "    print(metric(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CAISO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.read_csv(\"C:\\\\Users\\\\charles.gulian\\\\PycharmProjects\\\\RESCUE\\\\code\\\\data_for_heatmaps\\\\outputs_from_CAISO_histogram\\\\Flex_Ramp_Req_RTPD.csv\", index_col = [0])\n",
    "df_preds.index = pd.to_datetime(df_preds.index)\n",
    "\n",
    "df_outputs = pd.read_pickle(\"C:\\\\Users\\\\charles.gulian\\\\PycharmProjects\\\\RESCUE\\\\data\\\\rescue_v1_2\\\\output_trainval.pkl\")\n",
    "df_outputs = df_outputs.reindex_like(df_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9025537634408602\n",
      "803.9341397849462\n",
      "728.5136272499433\n",
      "176.30059722222228\n"
     ]
    }
   ],
   "source": [
    "month = 3\n",
    "\n",
    "y_true = df_outputs.loc[(df_outputs.index.year == 2019) & (df_outputs.index.month == month)]\n",
    "y_pred = df_preds.loc[(df_preds.index.year == 2019) & (df_preds.index.month == month), 'UP']\n",
    "\n",
    "for metric in [coverage, requirement, closeness, exceeding]:\n",
    "    print(metric(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==== User Defined inputs ====\n",
    "model_name = 'rescue_v1_2' # name of the model. Recommend to include version number\n",
    "#model_name = 'rescue_v1_1_no_calendar'\n",
    "\n",
    "# Target quantiles of the prediction \n",
    "PI_percentiles = np.array([0.975]) # quantiles to predict\n",
    "\n",
    "# Structural parameter of the ANN network\n",
    "num_neurons = 10\n",
    "activation_type = 'relu'\n",
    "\n",
    "# Cross validation parameters\n",
    "num_cv_folds = 10 # number of cross validation folds\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64 # size of each mini-batch in the SGD\n",
    "max_epochs = 50 # Maximum number of epochs in training. In each epoch, each training data is used exactly once\n",
    "optimizer_choice = 'adam' # Optimizer choice. Default to ADAM, a popular choice that have 1st and 2nd order momentum\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stop_monitor = 'val_loss' # The metrics to watch when deciding whether to stop. Recommendation: Validation loss as in 'val_loss'\n",
    "early_stop_min_delta = 0.5 # if the difference/decrease in loss is less than min_delta, the model is considered no longer improving\n",
    "early_stop_patience = 3 # For number of patience epochs, observe if the model has improved more than min_delta\n",
    "early_stop_verbosity = 1 # 0: no output, 1: some output, 2: full output\n",
    "\n",
    "# Check points parameters\n",
    "ckpt_monitor = 'val_loss' # Check points are only saved when there is an improvement in ckpt monitor\n",
    "\n",
    "# Losses and Metrics log parameters\n",
    "log_activation_freq = 0 # the frequency of logging hidden layer activation's histogram. Default to not log\n",
    "log_update_freq = 'epoch' # The frequency of logging. Default to record at the end of every epoch\n",
    "\n",
    "# Visualization parameters\n",
    "default_dpi = 300\n",
    "\n",
    "# define the mapping between internal feature name and feature label used in plotting\n",
    "label_to_feature_map = {\"Solar_RTPD_Forecast_T+1\":\"Solar Generation (MW)\",\n",
    "                       \"Wind_RTPD_Forecast_T+1\":\"Wind Generation (MW)\",\n",
    "                       \"Load_RTPD_Forecast_T+1\":\"Load (MW)\",\n",
    "                       \"Days_from_Start_Date_T+1\":\"Date of Observation\",\n",
    "                       \"Hour_Angle_T+1\":\"Hour\",\n",
    "                       \"Day_Angle_T+1\":\"Month\"}\n",
    "# Training and validation loss comparison for multiple folds\n",
    "x_jitter = 0.1\n",
    "metrics_to_idx_map = {'Loss (MW)':0, 'Coverage Probability (%)':1}\n",
    "metrics_to_compare = ['Loss (MW)', 'Coverage Probability (%)'] # choose in metrics_to_idx_map's keys\n",
    "\n",
    "\n",
    "# Time series\n",
    "ts_ranges = ['20170201','20170501','20170801','20171101'] # the example range of time series to plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NAbSZiaoJ4z"
   },
   "source": [
    "# 1. Data Ingestion\n",
    "\n",
    "Load in pre-processed trainval dataset that includes all input features/ output response for both training and validation. Prepare the cross validation splitting masks. Set up directory structure to store intermediate (`log`, `checkpoints`) and final outputs (`models`, `outputs`,`diagnostics`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day block shuffling pre-determnined....\n",
      "Done....\n",
      "Creating train val masks for each fold....\n",
      "Train and val masks are ready!\n"
     ]
    }
   ],
   "source": [
    "# Load in/ Create folder structure for the current model\n",
    "dir_str = utility.Dir_Structure(model_name = model_name)\n",
    "\n",
    "# Read in input and output of the training and validation samples from data pipeline. \n",
    "# This should be an output of the data_preprocessing script\n",
    "input_trainval = pd.read_pickle(dir_str.input_trainval_path)\n",
    "output_trainval = pd.read_pickle(dir_str.output_trainval_path)\n",
    "assert input_trainval.shape[0] == output_trainval.shape[0], \"Input and output shape mismatch!\"\n",
    "n_samples = input_trainval.shape[0]\n",
    "\n",
    "# Use cross validation script to conduct intra-day consecutive trainval splitting. The number of folds is \n",
    "# determined by num_cv_folds. The data of the same day would not end up separately in training and validation\n",
    "# to not overestimate model performance.\n",
    "val_masks_all_folds = cross_val.get_CV_masks(input_trainval.index, num_cv_folds, dir_str.shuffled_indices_path)\n",
    "\n",
    "# confirm the PI percentiles are symmetrical and 0.5 is one of the target quantile\n",
    "\n",
    "#for PI in PI_percentiles:\n",
    "#    assert np.allclose(1-PI_percentiles, PI_percentiles[::-1]), \"Not all PI intervals are constructed symmetrically!\"\n",
    "#assert 0.5 in PI_percentiles, \"Median forecast (P50) must be produced!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Definition\n",
    "Define the various component of the model that are active at various stage of the model life cycle. Before training: model structure, loss function. During training: call backs and metrics. After training: Saving functions. \n",
    "\n",
    "## 2.1 Model Structure.\n",
    "The RESCUE model is built with the Keras [functional API](https://www.tensorflow.org/guide/keras#model_subclassing). As it stands right now, it is a two layer ANN network with a pre-processing normalization layer. Rectified linear units is used as the activation function for the hidden layers, while the last layer is a direct linear regression.                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model that include the normalization layer\n",
    "inputs = tf.keras.Input(shape=input_trainval.shape[1:])\n",
    "\n",
    "# Create a Normalization layer and set its internal state using the training data\n",
    "normalizer = tf.keras.layers.experimental.preprocessing.Normalization(name = 'Normalization')\n",
    "norm_inputs = normalizer(inputs)\n",
    "\n",
    "# A two-layer ANN network for regression\n",
    "dense1 = tf.keras.layers.Dense(num_neurons, activation=activation_type)\n",
    "hidden1 = dense1(norm_inputs)\n",
    "dense2 = tf.keras.layers.Dense(num_neurons, activation=activation_type)\n",
    "hidden2 = dense2(hidden1)\n",
    "dense3 = tf.keras.layers.Dense(1)\n",
    "outputs = dense3(hidden2)\n",
    "\n",
    "# define model from inputs to outputs\n",
    "rescue_model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Loss Function\n",
    "\n",
    "The pinball loss function is used here to provide a quantile forecast rather than a median forecast. With increasing sample size, the pinball loss would converge to the quantile forecast of a variable conditional on the input variable. The quantile is given by a parameter `tau`, which is set in the user input as PI percentiles. \n",
    "\n",
    "One property of the pinball loss is that for 0% and 100% percentile, the pinball loss is always 0 no matter the model and parameter choice, while the median forecast have the highest loss. So comparing the losses across different quantiles makes little sense, and is advised against. For more information on pinball losses, check out [this wiki](https://en.wikipedia.org/wiki/Quantile_regression).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PinballLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, tau=0.5, name=\"pinball_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.tau = tau # the target quantile\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        err = y_true - y_pred # the convention is always true - pred\n",
    "        # essentially, quantile regression takes two region. For values bigger than the quantile forecast,\n",
    "        # they are weighted by 1-tau, while for values smaller than the forecast it's weighted by tau.\n",
    "        skewed_mse = tf.math.reduce_mean(tf.math.maximum(self.tau * err, (self.tau - 1) * err), axis=0)\n",
    "\n",
    "        return skewed_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Metrics\n",
    "\n",
    "In tensorflow terminology, metrics are quantities that are calculated as the training goes on to aid your judgement on model's fitness and completeness. In our use cases, we define two metrics for this purpose: coverage probability and average interval width.\n",
    "\n",
    "For prediction intervals, coverage probability refers to how often are the actual forecast included in the prediciton interval bands. Here we slightly modify the definition and refers to how often are the actual forecast smaller than the target quantile. For a well behaving model, the CP would converge to target quantile tau.\n",
    "\n",
    "For average interval width, it normally refers to the width of a prediction interval band. Again, we make a slight modification here. Since in practice, the forecast for quantiles above 50% are upwards reserve and normally positive, and below 50% are downwards reserve and normally negative, we are simply using the quantile forecast's absolute distance to 0 as the interval width here. For interpretation, we are looking for smaller requirement for better band width, but also high flexibitliy wrt to varying condidtions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoverageProbability(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name = 'CP',**kwargs):\n",
    "        super(CoverageProbability, self).__init__(name = name, **kwargs)\n",
    "        self.coverage_probability = self.add_weight(name = 'CP', initializer=\"zeros\", dtype = tf.float64)\n",
    "        # the cumulative number of samples and the number of samples smaller than current quantile forecast\n",
    "        self.cum_n_samples = self.add_weight(name = 'n_samples', initializer=\"zeros\", dtype = tf.int32)\n",
    "        self.cum_n_covered = self.add_weight(name = 'n_covered', initializer=\"zeros\", dtype = tf.int32)\n",
    "        \n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # the state would be updated everytime we have a new calculation\n",
    "        self.cum_n_samples.assign_add(tf.size(y_pred, out_type=tf.int32))\n",
    "        self.cum_n_covered.assign_add(tf.math.count_nonzero(tf.math.less_equal(y_true,y_pred), dtype = tf.int32))\n",
    "        # cp = n_covered/n_samples\n",
    "        self.coverage_probability.assign(tf.math.divide(self.cum_n_covered, self.cum_n_samples))\n",
    "\n",
    "    def result(self):\n",
    "        return self.coverage_probability\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.coverage_probability.assign(0.0)\n",
    "        \n",
    "class AverageIntervalWidth(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='AIW', **kwargs):\n",
    "        super(AverageIntervalWidth, self).__init__(name=name, **kwargs)\n",
    "        self.average_interval_width = self.add_weight(name='AIW', initializer=\"zeros\", dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # the state would be updated everytime we have a new calculation\n",
    "        self.average_interval_width.assign(tf.math.reduce_mean(y_pred))\n",
    "\n",
    "    def result(self):\n",
    "        return self.average_interval_width\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.average_interval_width.assign(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Callbacks\n",
    "\n",
    "In tensorflow terminology, callbacks are functions that get executed with certain frequency during training. For our purpose, all the callbacks happen once per epoch and we are using three types of callbacks: Early stopping, check points, and tensor boards. Saving the model is not necessarily a callback, but it also get executed once after the training for the model is complete, so is included in this segment.\n",
    "\n",
    "1. Early stopping stops the training when certain criteria is met. In general the criteria is that when `monitor` did not improve by more than `min_delta` in `patience` epoch, than the training is stopped. This is a way to effective prevent overfitting to the training data, so the monitor is most often validation loss.  \n",
    "2. Check points are periodical snap shot of parameter weights saved in case the model training is unexpectedly stopped.  We are saving the `checkpoints`,`logs`, and`trained models` in different folders for different taus and folds.\n",
    "3. Saving is very similar to check points. Difference is that check points are conducted at the end of every epoch, while saving only happens by the end of training session.\n",
    "4. The tensorboard callback allow us to visualize and observe losses and all metrics in a pre-compiled tensorboard interface. It can even visualize losses and metrics for multiple folds and taus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping stops the training when certain criteria is met.\n",
    "cb_early_stopping = tf.keras.callbacks.EarlyStopping(monitor=early_stop_monitor, min_delta=early_stop_min_delta, \n",
    "                                                     patience=early_stop_patience, verbose=early_stop_verbosity)\n",
    "\n",
    "# For the save best only parameter: we will overwrite the current checkpoint if and only if the `val_loss` \n",
    "# score has improved. Different fold and tau would end up in different ckpts_dir folder\n",
    "def get_cb_check_points(tau, fold_idx):\n",
    "    # make sure models for different tau go to different directories\n",
    "    ckpts_dir = os.path.join(dir_str.ckpts_dir, \"tau_{:.1%}\".format(tau),\"fold_#{}\".format(fold_idx))\n",
    "    if not os.path.exists(ckpts_dir):\n",
    "        os.makedirs(ckpts_dir)\n",
    "    cb_check_points = tf.keras.callbacks.ModelCheckpoint(filepath=ckpts_dir, save_best_only=True, monitor= ckpt_monitor, verbose=0)\n",
    "    return cb_check_points\n",
    "\n",
    "\n",
    "# Currrently not logging the histogram of activation and embedding layers. Write log per epoch.\n",
    "def get_cb_tensor_board(tau, fold_idx):\n",
    "    # make sure models for different tau and fold would get logged in different directory\n",
    "    logs_dir = os.path.join(dir_str.logs_dir, \"tau_{:.1%}\".format(tau),\"fold_#{}\".format(fold_idx))\n",
    "    if not os.path.exists(logs_dir):\n",
    "        os.makedirs(logs_dir)\n",
    "    cb_tensor_board = tf.keras.callbacks.TensorBoard(logs_dir, histogram_freq= log_activation_freq, \n",
    "                                                     embeddings_freq=0,  update_freq=log_update_freq)  \n",
    "    return cb_tensor_board\n",
    "\n",
    "# Save the model by the end of each training session. Might be replacible by checkpoints.\n",
    "def save_rescue_model(model, tau, fold_idx):\n",
    "    # make sure models for different tau and fold would get logged in different directory\n",
    "    models_dir = os.path.join(dir_str.models_dir, \"tau_{:.1%}\".format(tau),\"fold_#{}\".format(fold_idx))\n",
    "    if not os.path.exists(models_dir):\n",
    "        os.makedirs(models_dir)\n",
    "    \n",
    "    model.save(models_dir)\n",
    "    return None   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Training\n",
    "\n",
    "Training is the process where the parameter of a model changes to reduce some loss function. In our specific case, we are conducting training separtely for each target quantile and each fold. We first split data into training and validation based on current fold number, and then initialize a new model to fit to the training data until the pinball loss meet some stopping criteria, i.e. showing no significant decrease.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescue_model_set = {} # intialized container for TF models. Indexed by (tau, fold_idx)\n",
    "history = {} # intialized container for the training history of models. Indexed by (tau, fold_idx)\n",
    "\n",
    "# loop through different target quantiles and cross validation folds\n",
    "for tau in PI_percentiles:\n",
    "    print(\"Training model for Prediction interval: {:.1%}\".format(tau))\n",
    "    for fold_idx in range(num_cv_folds):\n",
    "        print(\"Cross Validation fold #\", fold_idx+1)\n",
    "        \n",
    "        # Split into training and validation dataset based on the CV validation masks generated in section 1.\n",
    "        input_train, output_train = input_trainval[~val_masks_all_folds[fold_idx]], output_trainval[~val_masks_all_folds[fold_idx]]\n",
    "        input_val, output_val = input_trainval[val_masks_all_folds[fold_idx]], output_trainval[val_masks_all_folds[fold_idx]]\n",
    "        \n",
    "        # retain value only and cast to 'float32'. Single precision calculate a lot faster than double precision.\n",
    "        input_train = input_train.values.astype('float32')\n",
    "        output_train = output_train.values.astype('float32')\n",
    "        input_val = input_val.values.astype('float32')\n",
    "        output_val = output_val.values.astype('float32')\n",
    "\n",
    "        # Using tf.data API to batch and shuffle the dataset. For shuffling, the buffer size should be bigger than the total \n",
    "        # sample count. Or else only the first buffle size of samples would be shuffled \n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((input_train, output_train)).shuffle(buffer_size= n_samples).batch(batch_size)\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((input_val, output_val)).shuffle(buffer_size = n_samples).batch(batch_size)\n",
    "\n",
    "        # Make a fresh clone of the rescue model for the specific quantile and fold\n",
    "        rescue_model_set[(tau, fold_idx)] = tf.keras.models.clone_model(rescue_model)\n",
    "        # For some layers the paramers are not trainable, and is adapted at the begining to data. E.g.:Normalization layer \n",
    "        rescue_model_set[(tau, fold_idx)].get_layer('Normalization').adapt(input_trainval.values)\n",
    "        # Compiling the loss, optimizer, metrics, and model into one compiled instance\n",
    "        rescue_model_set[(tau, fold_idx)].compile(loss = PinballLoss(tau = tau), optimizer=optimizer_choice,\n",
    "                                                  metrics= [CoverageProbability(), AverageIntervalWidth()])\n",
    "        \n",
    "        # The training process. Passing in callbacks to use in mid training \n",
    "        history[(tau, fold_idx)]= rescue_model_set[(tau, fold_idx)].fit(train_ds, validation_data=val_ds, epochs=max_epochs,\n",
    "                                                                        callbacks=[cb_early_stopping, get_cb_tensor_board(tau, fold_idx)])\n",
    "        \n",
    "        # Save the trained rescue model for each target percentile and fold\n",
    "        save_rescue_model(rescue_model_set[(tau, fold_idx)], tau, fold_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Inference\n",
    "After the model is trained, we use it to produce quantile predictions on the entire trainval set. Note that the training history is also transformed into and np array here for easy storage and visualizaiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize container for the inference result for different quantiles/CV folds\n",
    "multi_index_tau_folds = pd.MultiIndex.from_product([PI_percentiles, range(num_cv_folds)], names = ['Quantiles','Fold ID'])\n",
    "pred_trainval = pd.DataFrame(index = input_trainval.index, columns = multi_index_tau_folds) \n",
    "\n",
    "# Initialize container for the training loss and metric history\n",
    "num_metrics = len(history[(tau, fold_idx)].params['metrics'])\n",
    "training_history = np.ones((len(PI_percentiles), num_cv_folds, max_epochs, num_metrics))*np.nan\n",
    "\n",
    "# looping through all target percentiles and CV folds\n",
    "for i,tau in enumerate(PI_percentiles):\n",
    "    print ('Inferring on quantile of {:.1%}'.format(tau))\n",
    "    for fold_idx in range(num_cv_folds):\n",
    "        # Record training loss and metrics history\n",
    "        num_epochs = len(history[(tau, fold_idx)].epoch)\n",
    "        training_history[i,fold_idx,:num_epochs,:] = pd.DataFrame(history[(tau, fold_idx)].history).values\n",
    "        \n",
    "        # Deploy model on the trainval data and record inference results\n",
    "        pred_trainval.loc[:,(tau,fold_idx)] = rescue_model_set[(tau,fold_idx)].predict(input_trainval.values)\n",
    "        \n",
    "#Output inference result and training history to hard drive        \n",
    "pred_trainval.to_pickle(dir_str.pred_trainval_path)\n",
    "np.save(dir_str.training_hist_path, training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing models with Pareto efficiency framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in historical forecast errors and predictions from two models\n",
    "\n",
    "output_trainval = pd.read_pickle(dir_str.output_trainval_path)\n",
    "pred_trainval1 = pd.read_pickle('C:\\\\Users\\\\charles.gulian\\\\PycharmProjects\\\\RESCUE\\\\output\\\\rescue_v1_1\\\\pred_trainval.pkl')\n",
    "pred_trainval2 = pd.read_pickle('C:\\\\Users\\\\charles.gulian\\\\PycharmProjects\\\\RESCUE\\\\output\\\\rescue_v1_1_no_calendar\\\\pred_trainval.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.025, 0.05 , 0.25 , 0.5  , 0.75 , 0.95 , 0.975]),\n",
       " array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_trainval = pred_trainval1\n",
    "columns = pred_trainval.columns\n",
    "tau_arr = np.sort(np.unique(np.array([c[0] for c in columns])))\n",
    "CV_arr = np.unique(np.array([c[1] for c in columns]))\n",
    "tau_arr, CV_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(9).reshape(3,3)\n",
    "print(x)\n",
    "plt.pcolormesh(x)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV Fold ID</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lower Quantile</th>\n",
       "      <th>Upper Quantile</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">0.025</th>\n",
       "      <th>0.050</th>\n",
       "      <td>10294</td>\n",
       "      <td>10933</td>\n",
       "      <td>7857</td>\n",
       "      <td>9192</td>\n",
       "      <td>1852</td>\n",
       "      <td>11646</td>\n",
       "      <td>9134</td>\n",
       "      <td>10322</td>\n",
       "      <td>8898</td>\n",
       "      <td>20158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.250</th>\n",
       "      <td>421</td>\n",
       "      <td>408</td>\n",
       "      <td>61</td>\n",
       "      <td>366</td>\n",
       "      <td>73</td>\n",
       "      <td>232</td>\n",
       "      <td>716</td>\n",
       "      <td>399</td>\n",
       "      <td>146</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.500</th>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>39</td>\n",
       "      <td>37</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.750</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.950</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.975</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.050</th>\n",
       "      <th>0.250</th>\n",
       "      <td>794</td>\n",
       "      <td>752</td>\n",
       "      <td>676</td>\n",
       "      <td>947</td>\n",
       "      <td>1148</td>\n",
       "      <td>1548</td>\n",
       "      <td>2317</td>\n",
       "      <td>1291</td>\n",
       "      <td>1227</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.500</th>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>43</td>\n",
       "      <td>51</td>\n",
       "      <td>40</td>\n",
       "      <td>30</td>\n",
       "      <td>109</td>\n",
       "      <td>19</td>\n",
       "      <td>89</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.750</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.950</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.975</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">0.250</th>\n",
       "      <th>0.500</th>\n",
       "      <td>1605</td>\n",
       "      <td>1977</td>\n",
       "      <td>1467</td>\n",
       "      <td>704</td>\n",
       "      <td>871</td>\n",
       "      <td>1423</td>\n",
       "      <td>2229</td>\n",
       "      <td>1193</td>\n",
       "      <td>1035</td>\n",
       "      <td>1662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.750</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>232</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.950</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.975</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0.500</th>\n",
       "      <th>0.750</th>\n",
       "      <td>1162</td>\n",
       "      <td>185</td>\n",
       "      <td>1156</td>\n",
       "      <td>710</td>\n",
       "      <td>474</td>\n",
       "      <td>1762</td>\n",
       "      <td>2985</td>\n",
       "      <td>113</td>\n",
       "      <td>715</td>\n",
       "      <td>2166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.950</th>\n",
       "      <td>474</td>\n",
       "      <td>85</td>\n",
       "      <td>333</td>\n",
       "      <td>90</td>\n",
       "      <td>179</td>\n",
       "      <td>248</td>\n",
       "      <td>75</td>\n",
       "      <td>86</td>\n",
       "      <td>90</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.975</th>\n",
       "      <td>182</td>\n",
       "      <td>19</td>\n",
       "      <td>66</td>\n",
       "      <td>46</td>\n",
       "      <td>84</td>\n",
       "      <td>167</td>\n",
       "      <td>5</td>\n",
       "      <td>95</td>\n",
       "      <td>6</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.750</th>\n",
       "      <th>0.950</th>\n",
       "      <td>2961</td>\n",
       "      <td>1662</td>\n",
       "      <td>1659</td>\n",
       "      <td>709</td>\n",
       "      <td>1284</td>\n",
       "      <td>810</td>\n",
       "      <td>896</td>\n",
       "      <td>921</td>\n",
       "      <td>1004</td>\n",
       "      <td>2068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.975</th>\n",
       "      <td>959</td>\n",
       "      <td>512</td>\n",
       "      <td>282</td>\n",
       "      <td>377</td>\n",
       "      <td>529</td>\n",
       "      <td>514</td>\n",
       "      <td>157</td>\n",
       "      <td>627</td>\n",
       "      <td>310</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.950</th>\n",
       "      <th>0.975</th>\n",
       "      <td>11876</td>\n",
       "      <td>1790</td>\n",
       "      <td>4309</td>\n",
       "      <td>2840</td>\n",
       "      <td>9143</td>\n",
       "      <td>5207</td>\n",
       "      <td>7562</td>\n",
       "      <td>12020</td>\n",
       "      <td>4204</td>\n",
       "      <td>6103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "CV Fold ID                         0      1     2     3     4      5     6  \\\n",
       "Lower Quantile Upper Quantile                                                \n",
       "0.025          0.050           10294  10933  7857  9192  1852  11646  9134   \n",
       "               0.250             421    408    61   366    73    232   716   \n",
       "               0.500              47      4     6    23    10      6    39   \n",
       "               0.750               1      1     1     1     1      1     0   \n",
       "               0.950               0      0     0     0     0      0     0   \n",
       "               0.975               0      0     0     0     0      0     0   \n",
       "0.050          0.250             794    752   676   947  1148   1548  2317   \n",
       "               0.500              16     17    43    51    40     30   109   \n",
       "               0.750               0      1     1     1     0      2     2   \n",
       "               0.950               0      0     0     0     0      0     0   \n",
       "               0.975               0      0     0     0     0      0     0   \n",
       "0.250          0.500            1605   1977  1467   704   871   1423  2229   \n",
       "               0.750               1      0     0     2     1    232     2   \n",
       "               0.950               3      0     3     0    32      0     0   \n",
       "               0.975               0      0     0     0     2      0     0   \n",
       "0.500          0.750            1162    185  1156   710   474   1762  2985   \n",
       "               0.950             474     85   333    90   179    248    75   \n",
       "               0.975             182     19    66    46    84    167     5   \n",
       "0.750          0.950            2961   1662  1659   709  1284    810   896   \n",
       "               0.975             959    512   282   377   529    514   157   \n",
       "0.950          0.975           11876   1790  4309  2840  9143   5207  7562   \n",
       "\n",
       "CV Fold ID                         7     8      9  \n",
       "Lower Quantile Upper Quantile                      \n",
       "0.025          0.050           10322  8898  20158  \n",
       "               0.250             399   146    246  \n",
       "               0.500              37     9     10  \n",
       "               0.750               3     2      1  \n",
       "               0.950               0     0      0  \n",
       "               0.975               0     0      0  \n",
       "0.050          0.250            1291  1227    995  \n",
       "               0.500              19    89     33  \n",
       "               0.750               0     1      1  \n",
       "               0.950               0     0      0  \n",
       "               0.975               0     0      0  \n",
       "0.250          0.500            1193  1035   1662  \n",
       "               0.750               2     3      1  \n",
       "               0.950               0     2     10  \n",
       "               0.975               0     0      0  \n",
       "0.500          0.750             113   715   2166  \n",
       "               0.950              86    90    205  \n",
       "               0.975              95     6     59  \n",
       "0.750          0.950             921  1004   2068  \n",
       "               0.975             627   310    580  \n",
       "0.950          0.975           12020  4204   6103  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def n_crossings(pred_trainval):\n",
    "    columns = pred_trainval.columns\n",
    "    tau_arr = np.sort(np.unique(np.array([c[0] for c in columns]))) # Tau values need to be sorted\n",
    "    CV_arr = np.sort(np.unique(np.array([c[1] for c in columns]))) # CV fold IDs do not need to be sorted\n",
    "    \n",
    "    crossings = {}\n",
    "    \n",
    "    for CV in CV_arr:\n",
    "        # Look for quantile crossings only in models trained on same CV fold\n",
    "        crossings[CV] = {}\n",
    "        for i,t1 in enumerate(tau_arr):\n",
    "            for j,t2 in enumerate(tau_arr):\n",
    "                if t1 < t2:\n",
    "                    crossings[CV][(t1,t2)] = sum(pred_trainval[t1, CV] > pred_trainval[t2, CV]) # Record number of quantile crossings\n",
    "        \n",
    "    df = pd.DataFrame(crossings)\n",
    "    df.columns.name = 'CV Fold ID'\n",
    "    df.index.rename(['Lower Quantile', 'Upper Quantile'], inplace = True)\n",
    "    return df\n",
    "        \n",
    "\n",
    "df_crossings = n_crossings(pred_trainval1)\n",
    "df_crossings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Quantiles</th>\n",
       "      <th colspan=\"10\" halign=\"left\">0.975</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold ID</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coverage</th>\n",
       "      <td>0.975999</td>\n",
       "      <td>0.977888</td>\n",
       "      <td>0.977135</td>\n",
       "      <td>0.973146</td>\n",
       "      <td>0.977195</td>\n",
       "      <td>0.976753</td>\n",
       "      <td>0.976924</td>\n",
       "      <td>0.978591</td>\n",
       "      <td>0.976632</td>\n",
       "      <td>0.976723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>requirement</th>\n",
       "      <td>553.038</td>\n",
       "      <td>551.579</td>\n",
       "      <td>540.928</td>\n",
       "      <td>520.085</td>\n",
       "      <td>556.891</td>\n",
       "      <td>557.861</td>\n",
       "      <td>538.526</td>\n",
       "      <td>571.879</td>\n",
       "      <td>539.939</td>\n",
       "      <td>550.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exceeding</th>\n",
       "      <td>131.2</td>\n",
       "      <td>140.426</td>\n",
       "      <td>144.839</td>\n",
       "      <td>127.905</td>\n",
       "      <td>130.107</td>\n",
       "      <td>135.193</td>\n",
       "      <td>138.308</td>\n",
       "      <td>129.624</td>\n",
       "      <td>140.609</td>\n",
       "      <td>142.685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>closeness</th>\n",
       "      <td>523.352</td>\n",
       "      <td>521.806</td>\n",
       "      <td>511.569</td>\n",
       "      <td>490.972</td>\n",
       "      <td>526.842</td>\n",
       "      <td>528.163</td>\n",
       "      <td>508.926</td>\n",
       "      <td>541.446</td>\n",
       "      <td>510.528</td>\n",
       "      <td>521.398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_exceeding</th>\n",
       "      <td>1279.84</td>\n",
       "      <td>1376.21</td>\n",
       "      <td>1355.04</td>\n",
       "      <td>1371.95</td>\n",
       "      <td>1308.33</td>\n",
       "      <td>1245.53</td>\n",
       "      <td>1411.83</td>\n",
       "      <td>1215.6</td>\n",
       "      <td>1360.14</td>\n",
       "      <td>1268.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reserve_ramp_rate</th>\n",
       "      <td>152.873</td>\n",
       "      <td>160.912</td>\n",
       "      <td>168.575</td>\n",
       "      <td>196.709</td>\n",
       "      <td>166.073</td>\n",
       "      <td>142.732</td>\n",
       "      <td>164.337</td>\n",
       "      <td>168.053</td>\n",
       "      <td>174.775</td>\n",
       "      <td>154.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pinball_risk</th>\n",
       "      <td>16.0753</td>\n",
       "      <td>15.995</td>\n",
       "      <td>15.9354</td>\n",
       "      <td>15.5373</td>\n",
       "      <td>15.9898</td>\n",
       "      <td>16.1898</td>\n",
       "      <td>15.7552</td>\n",
       "      <td>16.1725</td>\n",
       "      <td>15.8846</td>\n",
       "      <td>16.1902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Quantiles             0.975                                                    \\\n",
       "Fold ID                   0         1         2         3         4         5   \n",
       "coverage           0.975999  0.977888  0.977135  0.973146  0.977195  0.976753   \n",
       "requirement         553.038   551.579   540.928   520.085   556.891   557.861   \n",
       "exceeding             131.2   140.426   144.839   127.905   130.107   135.193   \n",
       "closeness           523.352   521.806   511.569   490.972   526.842   528.163   \n",
       "max_exceeding       1279.84   1376.21   1355.04   1371.95   1308.33   1245.53   \n",
       "reserve_ramp_rate   152.873   160.912   168.575   196.709   166.073   142.732   \n",
       "pinball_risk        16.0753    15.995   15.9354   15.5373   15.9898   16.1898   \n",
       "\n",
       "Quantiles                                                  \n",
       "Fold ID                   6         7         8         9  \n",
       "coverage           0.976924  0.978591  0.976632  0.976723  \n",
       "requirement         538.526   571.879   539.939   550.738  \n",
       "exceeding           138.308   129.624   140.609   142.685  \n",
       "closeness           508.926   541.446   510.528   521.398  \n",
       "max_exceeding       1411.83    1215.6   1360.14   1268.51  \n",
       "reserve_ramp_rate   164.337   168.053   174.775   154.368  \n",
       "pinball_risk        15.7552   16.1725   15.8846   16.1902  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics = compute_metrics(output_trainval, pred_trainval1)\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_arr = [0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975]\n",
    "CV_folds = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = output_trainval\n",
    "\n",
    "CP_arr_1 = np.zeros(10)\n",
    "AIW_arr_1 = np.zeros(10)\n",
    "\n",
    "CP_arr_2 = np.zeros(10)\n",
    "AIW_arr_2 = np.zeros(10)\n",
    "\n",
    "# Metrics: mean requirement (MW), standard deviation of coverage (%)\n",
    "\n",
    "requirement_1 = np.zeros([len(tau_arr), len(CV_folds)])\n",
    "coverage_1 = np.zeros([len(tau_arr), len(CV_folds)])\n",
    "risk_1 = np.zeros([len(tau_arr), len(CV_folds)])\n",
    "\n",
    "requirement_2 = np.zeros([len(tau_arr), len(CV_folds)])\n",
    "coverage_2 = np.zeros([len(tau_arr), len(CV_folds)])\n",
    "risk_2 = np.zeros([len(tau_arr), len(CV_folds)])\n",
    "\n",
    "\n",
    "for i,tau in enumerate(tau_arr):\n",
    "    \n",
    "    for j,CV in enumerate(CV_folds):\n",
    "        \n",
    "        y_pred1 = pred_trainval1[(tau,CV)]\n",
    "        y_pred2 = pred_trainval2[(tau,CV)]\n",
    "\n",
    "        requirement_1[i,j] = requirement(y_true, y_pred1)\n",
    "        coverage_1[i,j] = coverage(y_true, y_pred1)\n",
    "        risk_1[i,j] = pinball_risk(y_true, y_pred1)\n",
    "        \n",
    "        requirement_2[i,j] = requirement(y_true, y_pred2)\n",
    "        coverage_2[i,j] = coverage(y_true, y_pred2)\n",
    "        risk_2[i,j] = pinball_risk(y_true, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto Comparison\n",
    "\n",
    "mean_AIW_1 = np.mean(requirement_1, axis = 1)\n",
    "std_coverage_1 = np.std(coverage_1, axis = 1)\n",
    "RMS_coverage_1 = np.sqrt(np.mean((coverage_1 - np.array(tau_arr).reshape(-1,1))**2, axis = 1))\n",
    "\n",
    "mean_AIW_2 = np.mean(requirement_2, axis = 1)\n",
    "std_coverage_2 = np.std(coverage_2, axis = 1)\n",
    "RMS_coverage_2 = np.sqrt(np.mean((coverage_2 - np.array(tau_arr).reshape(-1,1))**2, axis = 1))\n",
    "\n",
    "scale = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5]\n",
    "a_ = np.mean(np.abs(np.append(mean_AIW_1, mean_AIW_2)))\n",
    "b_ = 100*np.mean(np.append(std_coverage_1, std_coverage_2))\n",
    "for s in scale:\n",
    "    a = s*a_\n",
    "    b = s*b_\n",
    "\n",
    "    frontier_x = np.linspace(-a, a, 1000)\n",
    "    frontier_y = (b/a)*np.sqrt(a**2 - frontier_x**2)\n",
    "\n",
    "    plt.plot(frontier_x, frontier_y, 'k', linewidth = 0.5)\n",
    "    \n",
    "plt.scatter(mean_AIW_1, 100*RMS_coverage_1, alpha = 0.7, c = 'blue', label = \"v1.1\")\n",
    "plt.scatter(mean_AIW_2, 100*RMS_coverage_2, alpha = 0.7, c = 'orange', label = \"v1.1 - no calendar\")\n",
    "plt.plot(mean_AIW_1, 100*RMS_coverage_1, 'blue', linewidth = 0.75)\n",
    "plt.plot(mean_AIW_2, 100*RMS_coverage_2, 'orange', linewidth = 0.75)\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel('Mean Requirement (MW)')\n",
    "plt.ylabel('RMSE of Coverage (%)')\n",
    "plt.axis([-600, 600, 0.0, 2.0])\n",
    "plt.savefig(os.path.join('Pareto Comparison Plots', 'no_calendar_comparison.jpg'), dpi = 250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto Comparison\n",
    "\n",
    "mean_risk_1 = np.mean(risk_1, axis = 1)\n",
    "std_risk_1 = np.std(risk_1, axis = 1)\n",
    "\n",
    "mean_risk_2 = np.mean(risk_2, axis = 1)\n",
    "std_risk_2 = np.std(risk_2, axis = 1)\n",
    "\n",
    "mean_AIW_1 = np.mean(requirement_1, axis = 1)\n",
    "std_coverage_1 = np.std(coverage_1, axis = 1)\n",
    "\n",
    "mean_AIW_2 = np.mean(requirement_2, axis = 1)\n",
    "std_coverage_2 = np.std(coverage_2, axis = 1)\n",
    "    \n",
    "plt.scatter(mean_risk_1, 100*std_coverage_1, alpha = 0.7, c = 'blue', label = \"v1.1\")\n",
    "plt.scatter(mean_risk_2, 100*std_coverage_2, alpha = 0.7, c = 'orange', label = \"v1.1 - no calendar\")\n",
    "plt.plot(mean_risk_1, 100*std_coverage_1, 'blue', linewidth = 0.75)\n",
    "plt.plot(mean_risk_2, 100*std_coverage_2, 'orange', linewidth = 0.75)\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel('Pinball Loss (MW)')\n",
    "plt.ylabel('Standard Deviation of Coverage (%)')\n",
    "plt.savefig(os.path.join('Pareto Comparison Plots', 'no_calendar_comparison_CP_pinball_loss.jpg'), dpi = 250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto Comparison\n",
    "\n",
    "mean_risk_1 = np.mean(risk_1, axis = 1)\n",
    "std_risk_1 = np.std(risk_1, axis = 1)\n",
    "\n",
    "mean_risk_2 = np.mean(risk_2, axis = 1)\n",
    "std_risk_2 = np.std(risk_2, axis = 1)\n",
    "\n",
    "mean_AIW_1 = np.mean(requirement_1, axis = 1)\n",
    "std_coverage_1 = np.std(coverage_1, axis = 1)\n",
    "\n",
    "mean_AIW_2 = np.mean(requirement_2, axis = 1)\n",
    "std_coverage_2 = np.std(coverage_2, axis = 1)\n",
    "    \n",
    "plt.scatter(mean_risk_1, std_risk_1, alpha = 0.7, c = 'blue', label = \"v1.1\")\n",
    "plt.scatter(mean_risk_2, std_risk_2, alpha = 0.7, c = 'orange', label = \"v1.1 - no calendar\")\n",
    "plt.plot(mean_risk_1, std_risk_1, 'blue', linewidth = 0.75)\n",
    "plt.plot(mean_risk_2, std_risk_2, 'orange', linewidth = 0.75)\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel('Pinball Loss (MW)')\n",
    "plt.ylabel('Standard Deviation of Pinball Loss (MW)')\n",
    "plt.savefig(os.path.join('Pareto Comparison Plots', 'no_calendar_comparison_CP_pinball_loss.jpg'), dpi = 250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to show how standard deviation of CP relates to standard deviation of pinball loss (generalizability metrics)\n",
    "\n",
    "plt.scatter(std_risk_1, std_coverage_1, alpha = 0.7, c = 'blue')\n",
    "plt.scatter(std_risk_2, std_coverage_2, alpha = 0.7, c = 'orange')\n",
    "plt.xlabel('Standard Deviation of Pinball Loss (MW)')\n",
    "plt.ylabel('Standard Deviation of Coverage (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to show how standard deviation of CP relates to standard deviation of pinball loss (generalizability metrics)\n",
    "\n",
    "plt.scatter(std_risk_1, RMS_coverage_1, alpha = 0.7, c = 'blue')\n",
    "plt.scatter(std_risk_2, RMS_coverage_2, alpha = 0.7, c = 'orange')\n",
    "plt.xlabel('Standard Deviation of Pinball Loss (MW)')\n",
    "plt.ylabel('RMSE of Coverage (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Diagnostics and Visualizations\n",
    "This section includes an expanding set of diagnostics and visualization tool to assess the performance of RESCUE model, and check whether the model's behavior ahere to our intuition. As a standalone section, you should be able to skip **section 2-4** and directly start run **section 5** after **section 1**. First we load in the quantile prediction made in the inferrence section and the training history from the training section. They should all be on the hard drive already. from the hard drive if they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in quantile prediction\n",
    "if os.path.exists(dir_str.pred_trainval_path):\n",
    "    pred_trainval = pd.read_pickle(dir_str.pred_trainval_path)\n",
    "    pred_trainval = pred_trainval.xs(0, axis=1, level = 'Fold ID') # fold ID could really be any\n",
    "else:\n",
    "    print('No quantile prediction found! Run section 4 first!')\n",
    "# load in training history\n",
    "if os.path.exists(dir_str.training_hist_path):\n",
    "    training_hist = np.load(dir_str.training_hist_path)\n",
    "else:\n",
    "    print('No training history found!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Tensorboard\n",
    "Tensorboard is a built-in visualization tool to visualize the losses and metrics during multiple training processes. Use `tensorboard --logdir dir_str.logs_dir` to invoke it in the command line (replace `dir_str.logs_dir` with the folder it's referring to). This is also the reason we did not plot training history for each epoch, as it's done in tensorboard already\n",
    "\n",
    "**What to look for**: Training loss should converge in the last few epochs. Validation loss should not trail behind training loss by too much.\n",
    "\n",
    "\n",
    "## 5.2 Forecast uncertainty grouped by input features\n",
    "Visualize the forecast bias and uncertainty's change w.r.t to different input features. Check if the visualization corroborate or contradict our intuition. Many contradictions would constitute a red flag for model's fidelity.\n",
    "\n",
    "**What to expect:**\n",
    "1. **Forecast uncertainty v.s. Solar production**: Forecast uncertainty should be the highest for medium solar production, and low during both no solar and full solar periods.\n",
    "2. **Forecast uncertainty v.s. Wind production**:  Wind should have less effect on forecast uncertainty than solar. Increases with higher wind production\n",
    "3. **Forecast uncertainty v.s. Load**: Forecast uncertainty should increase with higher load.\n",
    "4. **Forecast uncertainty v.s. Hours**: Sunset and sunrise hours are associated with high amount of uncertainty. No solar night time hours should have low uncertainty.\n",
    "5. **Forecast uncertainty v.s. Season**: Highly dependent on local climate type. For mediterranean climate, expect little to no change, with a slight increase in winter. For continental climate, expect higher uncertainty in summer due to precipitation. For desert climate, expect smaller uncertainty year round, while expecting higher uncertainty year round for oceanic and tropical climates. For monsoon dominated climate, expect higher uncertainty in monsoon season (usually summer).\n",
    "6. **Forecast uncertainty v.s. Date of installation**: Highly dependent on modeled BA. If the BA experience significant renewable growth, expect increasing uncertainty with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the change in forecast uncertainty with respect to solar, wind and load\n",
    "for feature, label in label_to_feature_map.items():\n",
    "    # if looking at Hour and Month, binning is derived from index\n",
    "    if label == \"Hour\":\n",
    "        feature_discretized = pred_trainval.index.hour\n",
    "    elif label == 'Month':\n",
    "        feature_discretized = pred_trainval.index.month\n",
    "    else:\n",
    "        # else the label is derived from dicretizing a continuous input\n",
    "        feature_discretized = diagnostics.discretize_input(input_trainval[feature])\n",
    "        if label == 'Date of Observation':\n",
    "            # for 'Date of Observation', we add the starting date so it shows up as a historical data\n",
    "            feature_discretized = (pd.Timestamp(input_trainval.index[0].date()) \n",
    "                                   + feature_discretized.astype('int')*pd.Timedelta('1D'))\n",
    "    # plotting the uncertainty and bias for each feature bin\n",
    "    fig, axarr = diagnostics.plot_uncertainty_groupedby_feature(pred_trainval, feature_discretized, \n",
    "                                                                label_to_feature_map[feature])\n",
    "    \n",
    "    # special formatting of the x axis when we are using date observation\n",
    "    if label == 'Date of Observation':\n",
    "        fig.autofmt_xdate()\n",
    "    \n",
    "    fig.savefig(os.path.join(dir_str.diag_dir,label +'.png'), dpi = default_dpi) # save figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Model performance in different CV folds\n",
    "Visualizing model performance for all the cross validation folds across training and validation sets. An important tool in assessing the generalizability of the model.\n",
    "\n",
    "**What to look for:**\n",
    "1. Distance between training and validation. Smaller distance signifies a small amount of expected performance drop betweeen seen and unseen data, hence better generalizability.\n",
    "2. Spread among different validation folds: The smaller the spread, the more invariant the model is to its data, and hence better generalizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, _ =  diagnostics.plot_compare_train_val(training_hist, PI_percentiles,      \n",
    "                                             metrics_to_idx_map, metrics_to_compare, x_jitter =x_jitter)\n",
    "\n",
    "fig.savefig(os.path.join(dir_str.diag_dir, 'Train_Val_Comparison.png'), dpi=default_dpi, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Example Time series\n",
    "Visualize the quantile forecast for a few example days.\n",
    "\n",
    "\n",
    "**What to look for**:\n",
    "1. **Smoothness**: Is the reserve interval band changing continuously?\n",
    "2. **Coverage**: How good is the reserve interval band covering the true forecast error? The more the better.\n",
    "3. **Exceeding**: What kind of condition seems to lead to exceedance? Hopefully it should be extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = diagnostics.plot_example_ts(ts_ranges, pred_trainval, output_trainval)\n",
    "fig.savefig(os.path.join(dir_str.diag_dir,'Example_time_series.png'), dpi = default_dpi, bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ix4mEL65on-w"
   },
   "source": [
    "Archived code: Use `tf.GradientTape` for very fine grained control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIKdEzHAJGt7",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "# test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "\n",
    "# @tf.function\n",
    "# def train_step(inputs, outputs, model, loss_fn):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         # training=True is only needed if there are layers with different\n",
    "#         # behavior during training versus inference (e.g. Dropout).\n",
    "#         predictions = model(inputs, training=True)\n",
    "#         loss = loss_fn(outputs, predictions)\n",
    "#     gradients = tape.gradient(loss, rescue.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, rescue.trainable_variables))\n",
    "\n",
    "#     train_loss(loss)\n",
    "#     #train_accuracy(outputs, predictions)\n",
    "\n",
    "\n",
    "# @tf.function\n",
    "# def test_step(inputs, outputs, model, loss_fn):\n",
    "#     # training=False is only needed if there are layers with different\n",
    "#     # behavior during training versus inference (e.g. Dropout).\n",
    "#     predictions = rescue(inputs, training=False)\n",
    "#     t_loss = loss_object(outputs, predictions)\n",
    "\n",
    "#     test_loss(t_loss)\n",
    "#     #test_accuracy(outputs, predictions)\n",
    "\n",
    "# EPOCHS = 5\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#   # Reset the metrics at the start of the next epoch\n",
    "#   train_loss.reset_states()\n",
    "#   #train_accuracy.reset_states()\n",
    "#   test_loss.reset_states()\n",
    "#   #test_accuracy.reset_states()\n",
    "\n",
    "#   for inputs, outputs in train_ds:\n",
    "#     train_step(inputs, outputs)\n",
    "\n",
    "#   for test_inputs, test_outputs in test_ds:\n",
    "#     test_step(test_inputs, test_outputs)\n",
    "\n",
    "#   print(\n",
    "#     f'Epoch {epoch + 1}, '\n",
    "#     f'Loss: {train_loss.result()}, '\n",
    "#     #f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "#     f'Test Loss: {test_loss.result()}, '\n",
    "#     #f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "#   )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "advanced.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
