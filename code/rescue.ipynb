{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOsVdx6GGHmU"
   },
   "source": [
    "# RESCUE\n",
    "**Renewable Energy Salient Combined Uncertainty Estimator**\n",
    "\n",
    "## *Description:*\n",
    "A machine-learning based framework to quantify the short-term uncertainty in netload forecast developed by E3. \n",
    "The main strucutre of the model include a two layer artificial neural net with the pinball loss function as objective. Conditional on combinations of input, the model should be able to output quantile forecast for the net-load forecast error.\n",
    "\n",
    "This notebook contains the work load of ingesting pre-processed data, set up cross validation folds, training and deployment, and calling functions for diagnostics. For detailed implementation of data preprocessing or quoted functions, please refer to other script files. This project is available in the [e3/RESCUE](https://github.com/e3-/RESCUE) Github online repository. \n",
    "\n",
    "In the preliminary use case, the quantile forecast is trained on the response variable. For CAISO, as we are using RTPD forecast - RTD forecast as the response variable, the quantiles is actually on forecast difference rather than forecast error. Nevertheless, the model structure and the logic still holds the same.\n",
    "\n",
    "## *Highlights:*\n",
    "1. Incorporating a wide gamut of information: weather, calendar, forecast, and lagged error aware. \n",
    "2. Inherrent handles resource correlation as solar,wind, and load errors are co-trained within the model.\n",
    "3. Produces multiple prediction intervals for expected error in netload forecasting, for cherry picking down-stream\n",
    "4. Model agnostic. No requirement on knowledge of the inner workings of the netload forecast\n",
    "5. Adheres to best practice in statistics: cross validation, normalization, early-stopping, etc.\n",
    "\n",
    "## *To-dos:*\n",
    "1. Standardize the output for stability;\n",
    "\n",
    "## *Authors:* \n",
    "Yuchi Sun, Vignesh Venugopal, Charles Gulian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0trJmd6DjqBZ"
   },
   "outputs": [],
   "source": [
    "# Import third party packages\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Import self defined packages\n",
    "import cross_val\n",
    "import utility\n",
    "import diagnostics\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==== User Defined inputs ====\n",
    "model_name = 'rescue_v1_1_multi_objective' # name of the model. Recommend to include version number\n",
    "\n",
    "# Target quantiles of the prediction \n",
    "PI_percentiles = np.array([0.025, 0.05, 0.25, 0.5,0.75, 0.95, 0.975]) # quantiles to predict\n",
    "\n",
    "# Structural parameter of the ANN network\n",
    "num_neurons = 10\n",
    "activation_type = 'relu'\n",
    "\n",
    "# Cross validation parameters\n",
    "num_cv_folds = 10 # number of cross validation folds\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64 # size of each mini-batch in the SGD\n",
    "max_epochs = 50 # Maximum number of epochs in training. In each epoch, each training data is used exactly once\n",
    "optimizer_choice = 'adam' # Optimizer choice. Default to ADAM, a popular choice that have 1st and 2nd order momentum\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stop_monitor = 'val_loss' # The metrics to watch when deciding whether to stop. Recommendation: Validation loss as in 'val_loss'\n",
    "early_stop_min_delta = 0.5 # if the difference/decrease in loss is less than min_delta, the model is considered no longer improving\n",
    "early_stop_patience = 3 # For number of patience epochs, observe if the model has improved more than min_delta\n",
    "early_stop_verbosity = 1 # 0: no output, 1: some output, 2: full output\n",
    "\n",
    "# Check points parameters\n",
    "ckpt_monitor = 'val_loss' # Check points are only saved when there is an improvement in ckpt monitor\n",
    "\n",
    "# Losses and Metrics log parameters\n",
    "log_activation_freq = 0 # the frequency of logging hidden layer activation's histogram. Default to not log\n",
    "log_update_freq = 'epoch' # The frequency of logging. Default to record at the end of every epoch\n",
    "\n",
    "# Visualization parameters\n",
    "eg_fold_for_visualization = 0 # the example fold of the trained models to use for plotting model output\n",
    "default_dpi = 300 # dpi of the output graphics\n",
    "\n",
    "# define the mapping between internal feature name and feature label used in plotting\n",
    "label_to_feature_map = {\"Solar_RTPD_Forecast_T+1\":\"Solar Generation (MW)\",\n",
    "                       \"Wind_RTPD_Forecast_T+1\":\"Wind Generation (MW)\",\n",
    "                       \"Load_RTPD_Forecast_T+1\":\"Load (MW)\",\n",
    "                       \"Days_from_Start_Date_T+1\":\"Date of Observation\",\n",
    "                       \"Hour_Angle_T+1\":\"Hour\",\n",
    "                       \"Day_Angle_T+1\":\"Month\"}\n",
    "# Training and validation loss comparison for multiple folds\n",
    "x_jitter = 0.1\n",
    "metrics_to_idx_map = {'Loss (MW)':0, 'Coverage Probability (%)':1}\n",
    "metrics_to_compare = ['Loss (MW)', 'Coverage Probability (%)'] # choose in metrics_to_idx_map's keys\n",
    "\n",
    "\n",
    "# Time series\n",
    "ts_ranges = ['20170201','20170501','20170801','20171101'] # the example range of time series to plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NAbSZiaoJ4z"
   },
   "source": [
    "# 1. Data Ingestion\n",
    "\n",
    "Load in pre-processed trainval dataset that includes all input features/ output response for both training and validation. Prepare the cross validation splitting masks. Set up directory structure to store intermediate (`log`, `checkpoints`) and final outputs (`models`, `outputs`,`diagnostics`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in/ Create folder structure for the current model\n",
    "dir_str = utility.Dir_Structure(model_name = model_name)\n",
    "\n",
    "# Read in input and output of the training and validation samples from data pipeline. \n",
    "# This should be an output of the data_preprocessing script\n",
    "input_trainval = pd.read_pickle(dir_str.input_trainval_path)\n",
    "output_trainval = pd.read_pickle(dir_str.output_trainval_path)\n",
    "assert input_trainval.shape[0] == output_trainval.shape[0], \"Input and output shape mismatch!\"\n",
    "n_samples = input_trainval.shape[0]\n",
    "\n",
    "# Use cross validation script to conduct intra-day consecutive trainval splitting. The number of folds is \n",
    "# determined by num_cv_folds. The data of the same day would not end up separately in training and validation\n",
    "# to not overestimate model performance.\n",
    "val_masks_all_folds = cross_val.get_CV_masks(input_trainval.index, num_cv_folds, dir_str.shuffled_indices_path)\n",
    "\n",
    "# confirm the PI percentiles are symmetrical and 0.5 is one of the target quantile\n",
    "for PI in PI_percentiles:\n",
    "    assert np.allclose(1-PI_percentiles, PI_percentiles[::-1]), \"Not all PI intervals are constructed symmetrically!\"\n",
    "assert 0.5 in PI_percentiles, \"Median forecast (P50) must be produced!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Definition\n",
    "Define the various component of the model that are active at various stage of the model life cycle. Before training: model structure, loss function. During training: call backs and metrics. After training: Saving functions. \n",
    "\n",
    "## 2.1 Model Structure.\n",
    "The RESCUE model is built with the Keras [functional API](https://www.tensorflow.org/guide/keras#model_subclassing). As it stands right now, it is a two layer ANN network with a pre-processing normalization layer. Rectified linear units is used as the activation function for the hidden layers, while the last layer is a direct linear regression.                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model that include the normalization layer\n",
    "inputs = tf.keras.Input(shape=input_trainval.shape[1:])\n",
    "\n",
    "# Create a Normalization layer and set its internal state using the training data\n",
    "normalizer = tf.keras.layers.experimental.preprocessing.Normalization(name = 'Normalization')\n",
    "norm_inputs = normalizer(inputs)\n",
    "\n",
    "# A two-layer ANN network for regression\n",
    "dense1 = tf.keras.layers.Dense(num_neurons, activation=activation_type)\n",
    "hidden1 = dense1(norm_inputs)\n",
    "dense2 = tf.keras.layers.Dense(num_neurons, activation=activation_type)\n",
    "hidden2 = dense2(hidden1)\n",
    "dense3 = tf.keras.layers.Dense(1)\n",
    "outputs = dense3(hidden2)\n",
    "\n",
    "# define model from inputs to outputs\n",
    "rescue_model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Loss Function\n",
    "\n",
    "The pinball loss function is used here to provide a quantile forecast rather than a median forecast. With increasing sample size, the pinball loss would converge to the quantile forecast of a variable conditional on the input variable. The quantile is given by a parameter `tau`, which is set in the user input as PI percentiles. \n",
    "\n",
    "One property of the pinball loss is that for 0% and 100% percentile, the pinball loss is always 0 no matter the model and parameter choice, while the median forecast have the highest loss. So comparing the losses across different quantiles makes little sense, and is advised against. For more information on pinball losses, check out [this wiki](https://en.wikipedia.org/wiki/Quantile_regression).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PinballLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, tau=0.5, name=\"pinball_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.tau = tau # the target quantile\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        err = y_true - y_pred # the convention is always true - pred\n",
    "        # essentially, quantile regression takes two region. For values bigger than the quantile forecast,\n",
    "        # they are weighted by 1-tau, while for values smaller than the forecast it's weighted by tau.\n",
    "        skewed_mse = tf.math.reduce_mean(tf.math.maximum(self.tau * err, (self.tau - 1) * err), axis=0)\n",
    "\n",
    "        return skewed_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Metrics\n",
    "\n",
    "In tensorflow terminology, metrics are quantities that are calculated as the training goes on to aid your judgement on model's fitness and completeness. In our use cases, we define two metrics for this purpose: coverage probability and average interval width.\n",
    "\n",
    "For prediction intervals, **coverage probability** refers to how often are the actual forecast included in the prediciton interval bands. Here we slightly modify the definition and refers to how often are the actual forecast smaller than the target quantile. For a well behaving model, the CP would converge to target quantile tau.\n",
    "\n",
    "For **average interval width**, it normally refers to the width of a prediction interval band. Again, we make a slight modification here. Since in practice, the forecast for quantiles above 50% are upwards reserve and normally positive, and below 50% are downwards reserve and normally negative, we are simply using the quantile forecast's absolute distance to 0 as the interval width here. For interpretation, we are looking for smaller requirement for better band width, but also high flexibitliy wrt to varying condidtions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoverageProbability(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name = 'CP',**kwargs):\n",
    "        super(CoverageProbability, self).__init__(name = name, **kwargs)\n",
    "        self.coverage_probability = self.add_weight(name = 'CP', initializer=\"zeros\", dtype = tf.float64)\n",
    "        # the cumulative number of samples and the number of samples smaller than current quantile forecast\n",
    "        self.cum_n_samples = self.add_weight(name = 'n_samples', initializer=\"zeros\", dtype = tf.int32)\n",
    "        self.cum_n_covered = self.add_weight(name = 'n_covered', initializer=\"zeros\", dtype = tf.int32)\n",
    "        \n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # the state would be updated everytime we have a new calculation\n",
    "        self.cum_n_samples.assign_add(tf.size(y_pred, out_type=tf.int32))\n",
    "        self.cum_n_covered.assign_add(tf.math.count_nonzero(tf.math.less_equal(y_true,y_pred), dtype = tf.int32))\n",
    "        # cp = n_covered/n_samples\n",
    "        self.coverage_probability.assign(tf.math.divide(self.cum_n_covered, self.cum_n_samples))\n",
    "\n",
    "    def result(self):\n",
    "        return self.coverage_probability\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.coverage_probability.assign(0.0)\n",
    "        \n",
    "class AverageIntervalWidth(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='AIW', **kwargs):\n",
    "        super(AverageIntervalWidth, self).__init__(name=name, **kwargs)\n",
    "        self.average_interval_width = self.add_weight(name='AIW', initializer=\"zeros\", dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # the state would be updated everytime we have a new calculation\n",
    "        self.average_interval_width.assign(tf.math.reduce_mean(y_pred))\n",
    "\n",
    "    def result(self):\n",
    "        return self.average_interval_width\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.average_interval_width.assign(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Callbacks\n",
    "\n",
    "In tensorflow terminology, callbacks are functions that get executed with certain frequency during training. For our purpose, all the callbacks happen once per epoch and we are using three types of callbacks: Early stopping, check points, and tensor boards. Saving the model is not necessarily a callback, but it also get executed once after the training for the model is complete, so is included in this segment.\n",
    "\n",
    "1. Early stopping stops the training when certain criteria is met. In general the criteria is that when `monitor` did not improve by more than `min_delta` in `patience` epoch, than the training is stopped. This is a way to effective prevent overfitting to the training data, so the monitor is most often validation loss.  \n",
    "2. Check points are periodical snap shot of parameter weights saved in case the model training is unexpectedly stopped.  We are saving the `checkpoints`,`logs`, and`trained models` in different folders for different taus and folds.\n",
    "3. Saving is very similar to check points. Difference is that check points are conducted at the end of every epoch, while saving only happens by the end of training session.\n",
    "4. The tensorboard callback allow us to visualize and observe losses and all metrics in a pre-compiled tensorboard interface. It can even visualize losses and metrics for multiple folds and taus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping stops the training when certain criteria is met.\n",
    "cb_early_stopping = tf.keras.callbacks.EarlyStopping(monitor=early_stop_monitor, min_delta=early_stop_min_delta, \n",
    "                                                     patience=early_stop_patience, verbose=early_stop_verbosity)\n",
    "\n",
    "# For the save best only parameter: we will overwrite the current checkpoint if and only if the `val_loss` \n",
    "# score has improved. Different fold and tau would end up in different ckpts_dir folder\n",
    "def get_cb_check_points(tau, fold_idx):\n",
    "    # make sure models for different tau go to different directories\n",
    "    ckpts_dir = os.path.join(dir_str.ckpts_dir, \"tau_{:.1%}\".format(tau),\"fold_#{}\".format(fold_idx))\n",
    "    if not os.path.exists(ckpts_dir):\n",
    "        os.makedirs(ckpts_dir)\n",
    "    cb_check_points = tf.keras.callbacks.ModelCheckpoint(filepath=ckpts_dir, save_best_only=True, monitor= ckpt_monitor, verbose=0)\n",
    "    return cb_check_points\n",
    "\n",
    "\n",
    "# Currrently not logging the histogram of activation and embedding layers. Write log per epoch.\n",
    "def get_cb_tensor_board(tau, fold_idx):\n",
    "    # make sure models for different tau and fold would get logged in different directory\n",
    "    logs_dir = os.path.join(dir_str.logs_dir, \"tau_{:.1%}\".format(tau),\"fold_#{}\".format(fold_idx))\n",
    "    if not os.path.exists(logs_dir):\n",
    "        os.makedirs(logs_dir)\n",
    "    cb_tensor_board = tf.keras.callbacks.TensorBoard(logs_dir, histogram_freq= log_activation_freq, \n",
    "                                                     embeddings_freq=0,  update_freq=log_update_freq)  \n",
    "    return cb_tensor_board\n",
    "\n",
    "# Save the model by the end of each training session. Might be replacible by checkpoints.\n",
    "def save_rescue_model(model, tau, fold_idx):\n",
    "    # make sure models for different tau and fold would get logged in different directory\n",
    "    models_dir = os.path.join(dir_str.models_dir, \"tau_{:.1%}\".format(tau),\"fold_#{}\".format(fold_idx))\n",
    "    if not os.path.exists(models_dir):\n",
    "        os.makedirs(models_dir)\n",
    "    \n",
    "    model.save(models_dir)\n",
    "    return None   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Training\n",
    "\n",
    "Training is the process where the parameter of a model changes to reduce some loss function. In our specific case, we are conducting training separtely for each target quantile and each fold. We first split data into training and validation based on current fold number, and then initialize a new model to fit to the training data until the pinball loss meet some stopping criteria, i.e. showing no significant decrease.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescue_model_set = {} # intialized container for TF models. Indexed by (tau, fold_idx)\n",
    "history = {} # intialized container for the training history of models. Indexed by (tau, fold_idx)\n",
    "\n",
    "# loop through different target quantiles and cross validation folds\n",
    "for tau in PI_percentiles:\n",
    "    print(\"Training model for Prediction interval: {:.1%}\".format(tau))\n",
    "    for fold_idx in range(num_cv_folds):\n",
    "        print(\"Cross Validation fold #\", fold_idx+1)\n",
    "        \n",
    "        # Split into training and validation dataset based on the CV validation masks generated in section 1.\n",
    "        input_train, output_train = input_trainval[~val_masks_all_folds[fold_idx]], output_trainval[~val_masks_all_folds[fold_idx]]\n",
    "        input_val, output_val = input_trainval[val_masks_all_folds[fold_idx]], output_trainval[val_masks_all_folds[fold_idx]]\n",
    "        \n",
    "        # retain value only and cast to 'float32'. Single precision calculate a lot faster than double precision.\n",
    "        input_train = input_train.values.astype('float32')\n",
    "        output_train = output_train.values.astype('float32')\n",
    "        input_val = input_val.values.astype('float32')\n",
    "        output_val = output_val.values.astype('float32')\n",
    "\n",
    "        # Using tf.data API to batch and shuffle the dataset. For shuffling, the buffer size should be bigger than the total \n",
    "        # sample count. Or else only the first buffle size of samples would be shuffled \n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((input_train, output_train)).shuffle(buffer_size= n_samples).batch(batch_size)\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((input_val, output_val)).shuffle(buffer_size = n_samples).batch(batch_size)\n",
    "\n",
    "        # Make a fresh clone of the rescue model for the specific quantile and fold\n",
    "        rescue_model_set[(tau, fold_idx)] = tf.keras.models.clone_model(rescue_model)\n",
    "        # For some layers the paramers are not trainable, and is adapted at the begining to data. E.g.:Normalization layer \n",
    "        rescue_model_set[(tau, fold_idx)].get_layer('Normalization').adapt(input_trainval.values)\n",
    "        # Compiling the loss, optimizer, metrics, and model into one compiled instance\n",
    "        rescue_model_set[(tau, fold_idx)].compile(loss = PinballLoss(tau = tau), optimizer=optimizer_choice,\n",
    "                                                  metrics= [CoverageProbability(),AverageIntervalWidth()])\n",
    "        \n",
    "        # The training process. Passing in callbacks to use in mid training \n",
    "        history[(tau, fold_idx)]= rescue_model_set[(tau, fold_idx)].fit(train_ds, validation_data=val_ds, epochs=max_epochs,\n",
    "                                                                        callbacks=[cb_early_stopping, get_cb_tensor_board(tau, fold_idx)])\n",
    "        \n",
    "        # Save the trained rescue model for each target percentile and fold\n",
    "        save_rescue_model(rescue_model_set[(tau, fold_idx)], tau, fold_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Inference\n",
    "After the model is trained, we use it to produce quantile predictions on the entire trainval set. Note that the training history is also transformed into and np array here for easy storage and visualizaiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize container for the inference result for different quantiles/CV folds\n",
    "multi_index_tau_folds = pd.MultiIndex.from_product([PI_percentiles, range(num_cv_folds)], names = ['Quantiles','Fold ID'])\n",
    "pred_trainval = pd.DataFrame(index = input_trainval.index, columns = multi_index_tau_folds) \n",
    "\n",
    "# Initialize container for the training loss and metric history\n",
    "num_metrics = len(history[(tau, fold_idx)].params['metrics'])\n",
    "training_history = np.ones((len(PI_percentiles), num_cv_folds, max_epochs, num_metrics))*np.nan\n",
    "\n",
    "# looping through all target percentiles and CV folds\n",
    "for i,tau in enumerate(PI_percentiles):\n",
    "    print ('Inferring on quantile of {:.1%}'.format(tau))\n",
    "    for fold_idx in range(num_cv_folds):\n",
    "        # Record training loss and metrics history\n",
    "        num_epochs = len(history[(tau, fold_idx)].epoch)\n",
    "        training_history[i,fold_idx,:num_epochs,:] = pd.DataFrame(history[(tau, fold_idx)].history).values\n",
    "        \n",
    "        # Deploy model on the trainval data and record inference results\n",
    "        pred_trainval.loc[:,(tau,fold_idx)] = rescue_model_set[(tau,fold_idx)].predict(input_trainval.values)\n",
    "        \n",
    "#Output inference result and training history to hard drive        \n",
    "pred_trainval.to_pickle(dir_str.pred_trainval_path)\n",
    "np.save(dir_str.training_hist_path, training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Diagnostics and Visualizations\n",
    "This section includes an expanding set of diagnostics and visualization tool to assess the performance of RESCUE model, and check whether the model's behavior ahere to our intuition. As a standalone section, you should be able to skip **section 2-4** and directly start run **section 5** after **section 1**. First we load in the quantile prediction made in the inference section and the training history from the training section. They should all be on the hard drive already. from the hard drive if they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in quantile prediction\n",
    "if os.path.exists(dir_str.pred_trainval_path):\n",
    "    pred_trainval = pd.read_pickle(dir_str.pred_trainval_path)\n",
    "else:\n",
    "    print('No quantile prediction found! Run section 4 first!')\n",
    "# load in training history\n",
    "if os.path.exists(dir_str.training_hist_path):\n",
    "    training_hist = np.load(dir_str.training_hist_path)\n",
    "else:\n",
    "    print('No training history found!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Numeric Metrics\n",
    "The sections below contains numeric metrics generated for the RESCUE model.\n",
    "### 5.1.1 Metrics matrix\n",
    "For the metrics matrix, it places different prediction interval on different columns and different metrics on the rows. The metrics currently includes:\n",
    "- coverage\n",
    "- requirement\n",
    "- exceeding\n",
    "- closeness\n",
    "- max exceedance\n",
    "- reserve ramp rate\n",
    "- pinball loss\n",
    "\n",
    "The terminology for the metrics are largely following [CAISO FRP](http://www.caiso.com/InitiativeDocuments/AppendixC-QuantileRegressionApproach-FlexibleRampingProductRequirements.pdf) convention. For more information, please refer to the documentation within metrics module for each of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.compute_metrics_for_all_taus(output_trainval, pred_trainval, val_masks_all_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Quantile crossing\n",
    "Theoretically, for any type of forecast error, larger quantiles should always have a higher(more positive) numeric value. However, when this is not the case with two quantiles, it is called an instance of quantile crossing. This section characterizes the numbers of occurence for this phenomenom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.n_crossings(pred_trainval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Tensorboard\n",
    "Tensorboard is a built-in visualization tool to visualize the losses and metrics during multiple training processes. Use `tensorboard --logdir dir_str.logs_dir` to invoke it in the command line (replace `dir_str.logs_dir` with the folder it's referring to). This is also the reason we did not plot training history for each epoch, as it's done in tensorboard already\n",
    "\n",
    "**What to look for**: Training loss should converge in the last few epochs. Validation loss should not trail behind training loss by too much.\n",
    "\n",
    "## 5.3 Visualization\n",
    "The sections below contains visualization of the various output of the RESCUE model. Since visualizing multiple folds' output at the same time brings visual chaos, we use an example fold from the trained models as the basis for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_trainval = pred_trainval.xs(eg_fold_for_visualization, axis=1, level = 'Fold ID') # fold ID could really be any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Forecast uncertainty grouped by input features\n",
    "Visualize the forecast bias and uncertainty's change w.r.t to different input features. Check if the visualization corroborate or contradict our intuition. Many contradictions would constitute a red flag for model's fidelity.\n",
    "\n",
    "**What to expect:**\n",
    "1. **Forecast uncertainty v.s. Solar production**: Forecast uncertainty should be the highest for medium solar production, and low during both no solar and full solar periods.\n",
    "2. **Forecast uncertainty v.s. Wind production**:  Wind should have less effect on forecast uncertainty than solar. Increases with higher wind production\n",
    "3. **Forecast uncertainty v.s. Load**: Forecast uncertainty should increase with higher load.\n",
    "4. **Forecast uncertainty v.s. Hours**: Sunset and sunrise hours are associated with high amount of uncertainty. No solar night time hours should have low uncertainty.\n",
    "5. **Forecast uncertainty v.s. Season**: Highly dependent on local climate type. For mediterranean climate, expect little to no change, with a slight increase in winter. For continental climate, expect higher uncertainty in summer due to precipitation. For desert climate, expect smaller uncertainty year round, while expecting higher uncertainty year round for oceanic and tropical climates. For monsoon dominated climate, expect higher uncertainty in monsoon season (usually summer).\n",
    "6. **Forecast uncertainty v.s. Date of installation**: Highly dependent on modeled BA. If the BA experience significant renewable growth, expect increasing uncertainty with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the change in forecast uncertainty with respect to Solar, wind and load\n",
    "for feature, label in label_to_feature_map.items():\n",
    "    # if looking at Hour and Month, binning is derived from index\n",
    "    if label == \"Hour\":\n",
    "        feature_discretized = pred_trainval.index.hour\n",
    "    elif label == 'Month':\n",
    "        feature_discretized = pred_trainval.index.month\n",
    "    else:\n",
    "        # else the label is derived from dicretizing a continuous input\n",
    "        feature_discretized = diagnostics.discretize_input(input_trainval[feature])\n",
    "        if label == 'Date of Observation':\n",
    "            # for 'Date of Observation', we add the starting date so it shows up as a historical data\n",
    "            feature_discretized = (pd.Timestamp(input_trainval.index[0].date()) \n",
    "                                   + feature_discretized.astype('int')*pd.Timedelta('1D'))\n",
    "    # plotting the uncertainty and bias for each feature bin\n",
    "    fig, axarr = diagnostics.plot_uncertainty_groupedby_feature(pred_trainval, feature_discretized, \n",
    "                                                                label_to_feature_map[feature])\n",
    "    \n",
    "    # special formatting of the x axis when we are using date observation\n",
    "    if label == 'Date of Observation':\n",
    "        fig.autofmt_xdate()\n",
    "    \n",
    "    fig.savefig(os.path.join(dir_str.diag_dir,label +'.png'), dpi = default_dpi) # save figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Model performance in different CV folds\n",
    "Visualizing model performance for all the cross validation folds across training and validation sets. An important tool in assessing the generalizability of the model.\n",
    "\n",
    "**What to look for:**\n",
    "1. Distance between training and validation. Smaller distance signifies a small amount of expected performance drop betweeen seen and unseen data, hence better generalizability.\n",
    "2. Spread among different validation folds: The smaller the spread, the more invariant the model is to its data, and hence better generalizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, _ =  diagnostics.plot_compare_train_val(training_hist, PI_percentiles,      \n",
    "                                             metrics_to_idx_map, metrics_to_compare, x_jitter =x_jitter)\n",
    "\n",
    "fig.savefig(os.path.join(dir_str.diag_dir, 'Train_Val_Comparison.png'), dpi=default_dpi, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 Example Time series\n",
    "Visualize the quantile forecast for a few example days.\n",
    "\n",
    "\n",
    "**What to look for**:\n",
    "1. **Smoothness**: Is the reserve interval band changing continuously?\n",
    "2. **Coverage**: How good is the reserve interval band covering the true forecast error? The more the better.\n",
    "3. **Exceeding**: What kind of condition seems to lead to exceedance? Hopefully it should be extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = diagnostics.plot_example_ts(ts_ranges, pred_trainval, output_trainval)\n",
    "fig.savefig(os.path.join(dir_str.diag_dir,'Example_time_series.png'), dpi = default_dpi, bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ix4mEL65on-w"
   },
   "source": [
    "Archived code: Use `tf.GradientTape` for very fine grained control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIKdEzHAJGt7"
   },
   "outputs": [],
   "source": [
    "# train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "# test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "\n",
    "# @tf.function\n",
    "# def train_step(inputs, outputs, model, loss_fn):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         # training=True is only needed if there are layers with different\n",
    "#         # behavior during training versus inference (e.g. Dropout).\n",
    "#         predictions = model(inputs, training=True)\n",
    "#         loss = loss_fn(outputs, predictions)\n",
    "#     gradients = tape.gradient(loss, rescue.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, rescue.trainable_variables))\n",
    "\n",
    "#     train_loss(loss)\n",
    "#     #train_accuracy(outputs, predictions)\n",
    "\n",
    "\n",
    "# @tf.function\n",
    "# def test_step(inputs, outputs, model, loss_fn):\n",
    "#     # training=False is only needed if there are layers with different\n",
    "#     # behavior during training versus inference (e.g. Dropout).\n",
    "#     predictions = rescue(inputs, training=False)\n",
    "#     t_loss = loss_object(outputs, predictions)\n",
    "\n",
    "#     test_loss(t_loss)\n",
    "#     #test_accuracy(outputs, predictions)\n",
    "\n",
    "# EPOCHS = 5\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#   # Reset the metrics at the start of the next epoch\n",
    "#   train_loss.reset_states()\n",
    "#   #train_accuracy.reset_states()\n",
    "#   test_loss.reset_states()\n",
    "#   #test_accuracy.reset_states()\n",
    "\n",
    "#   for inputs, outputs in train_ds:\n",
    "#     train_step(inputs, outputs)\n",
    "\n",
    "#   for test_inputs, test_outputs in test_ds:\n",
    "#     test_step(test_inputs, test_outputs)\n",
    "\n",
    "#   print(\n",
    "#     f'Epoch {epoch + 1}, '\n",
    "#     f'Loss: {train_loss.result()}, '\n",
    "#     #f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "#     f'Test Loss: {test_loss.result()}, '\n",
    "#     #f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "#   )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "advanced.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
