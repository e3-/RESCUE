{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOsVdx6GGHmU"
   },
   "source": [
    "# RESCUE\n",
    "*Renewable Energy Salient Combined Uncertainty Estimator*\n",
    "\n",
    "A machine-learning based framework to quantify the short-term uncertainty in netload forecast developed by E3.\n",
    "\n",
    "Characteristics includes:\n",
    "1. Weather, time and recent errors aware.\n",
    "2. Correlation aware as solar,wind, and load errors are co-trained.\n",
    "3. Produces multiple prediction intervals for expected error in netload forecasting\n",
    "4. Model agnostic. No requirement on knowledge of the inner workings of the netload forecast\n",
    "\n",
    "Authors: Yuchi Sun, Vignesh Venugopal, Charles Gulian, Huai Jiang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0trJmd6DjqBZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cross_val\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_trainval = pd.read_pickle(os.path.join('output','pred_trainval.pkl'))\n",
    "output_trainval = pd.read_pickle(os.path.join('outputs_from_code','trainval_output.pkl')).T\n",
    "datetimes_trainval = pd.read_pickle(os.path.join('outputs_from_code','trainval_datetimes.pkl')).T\n",
    "#output_trainval.set_index(pd.to_datetime(datetimes_trainval['T0']), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAISO = pd.read_csv('CAISO_measurements.csv', index_col = 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_data = pd.read_pickle(os.path.join('outputs_from_code', 'metrics_data_v2.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = metrics_data.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAISO Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measurements reported by CAISO for Histogram Approach:\n",
      "\n",
      "Coverage: 0.9671%\n",
      "Requirement: 602.85 MW\n",
      "Closeness: 595.46 MW\n",
      "Exceeding: 175.07 MW\n",
      "\n",
      "Measurements reported by CAISO for Quantile Regression Approach:\n",
      "\n",
      "Coverage: 0.968%\n",
      "Requirement: 547.13 MW\n",
      "Closeness: 540.99 MW\n",
      "Exceeding: 163.74 MW\n"
     ]
    }
   ],
   "source": [
    "print('Measurements reported by CAISO for Histogram Approach:\\n')\n",
    "\n",
    "print('Coverage: {}%'.format(CAISO['Coverage']['Histogram']))\n",
    "print('Requirement: {} MW'.format(CAISO['Requirement']['Histogram']))\n",
    "print('Closeness: {} MW'.format(CAISO['Closeness']['Histogram']))\n",
    "print('Exceeding: {} MW'.format(CAISO['Exceeding']['Histogram']))\n",
    "\n",
    "print('\\nMeasurements reported by CAISO for Quantile Regression Approach:\\n')\n",
    "\n",
    "print('Coverage: {}%'.format(CAISO['Coverage']['Quantile Regression']))\n",
    "print('Requirement: {} MW'.format(CAISO['Requirement']['Quantile Regression']))\n",
    "print('Closeness: {} MW'.format(CAISO['Closeness']['Quantile Regression']))\n",
    "print('Exceeding: {} MW'.format(CAISO['Exceeding']['Quantile Regression']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to RESCUE model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 97.64514411436723%\n",
      "Requirement: 584.8646850585938 MW\n",
      "Closeness: 555.9745483398438 MW\n",
      "Exceeding: 150.59970092773438 MW\n",
      "\n",
      "\n",
      "Max Exceeding: 1295.1522216796875 MW\n",
      "Average reserve ramp rate: 124.8332290649414 MW/hr\n"
     ]
    }
   ],
   "source": [
    "#q1, q2 = 0.0125, 0.9875\n",
    "q1, q2 = 0.025, 0.975\n",
    "    \n",
    "y_true = np.concatenate([d[(q1, CV)]['y_true'] for CV in range(10)]).reshape(-1,1)\n",
    "y_pred = np.concatenate([d[(q2, CV)]['y_pred'] for CV in range(10)])\n",
    "\n",
    "print('Coverage: {}%'.format(100*coverage(y_true, y_pred)))\n",
    "print('Requirement: {} MW'.format(requirement(y_true, y_pred)))\n",
    "print('Closeness: {} MW'.format(closeness(y_true, y_pred)))\n",
    "print('Exceeding: {} MW'.format(exceeding(y_true, y_pred)))\n",
    "\n",
    "print('\\n')\n",
    "print('Max Exceeding: {} MW'.format(max_exceeding(y_true, y_pred)))\n",
    "print('Average reserve ramp rate: {} MW/hr'.format(reserve_ramp_rate(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to naive histogram approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.9749947256854098%\n",
      "Requirement: 726.583676147461 MW\n",
      "Closeness: 699.3185072106455 MW\n",
      "Exceeding: 174.31967962379196 MW\n",
      "\n",
      "\n",
      "Max Exceeding: 1425.466372680664 MW\n",
      "Average reserve ramp rate: 0.0 MW/hr\n"
     ]
    }
   ],
   "source": [
    "y_hist = np.percentile(y_true, q2*100)*np.ones([len(y_true),1])\n",
    "\n",
    "print('Coverage: {}%'.format(coverage(y_true, y_hist)))\n",
    "print('Requirement: {} MW'.format(requirement(y_true, y_hist)))\n",
    "print('Closeness: {} MW'.format(closeness(y_true, y_hist)))\n",
    "print('Exceeding: {} MW'.format(exceeding(y_true, y_hist)))\n",
    "\n",
    "print('\\n')\n",
    "print('Max Exceeding: {} MW'.format(max_exceeding(y_true, y_hist)))\n",
    "print('Average reserve ramp rate: {} MW/hr'.format(reserve_ramp_rate(y_true, y_hist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to 40-day histogram approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.9739599553943681%\n",
      "Requirement: 723.1464795288373 MW\n",
      "Closeness: 696.2136452160092 MW\n",
      "Exceeding: 173.7738467022223 MW\n",
      "\n",
      "\n",
      "Max Exceeding: 1366.1629669189451 MW\n",
      "Average reserve ramp rate: 0.4467771998356034 MW/hr\n"
     ]
    }
   ],
   "source": [
    "horizon = 4*24*40 # 15 minute observations for 40 days = 4*24*40 = 3840\n",
    "y_hist40 = np.zeros([len(y_true),1])\n",
    "for i in range(1, len(y_true)):\n",
    "    y_hist40[i] = np.percentile(y_true[max(0, i-horizon):i], q2*100)\n",
    "#y_hist40[4*24*5] = np.mean(y_hist40)*np.ones([4*24*5,1]) # First 5 days are just the average of the array\n",
    "\n",
    "print('Coverage: {}%'.format(coverage(y_true, y_hist40)))\n",
    "print('Requirement: {} MW'.format(requirement(y_true, y_hist40)))\n",
    "print('Closeness: {} MW'.format(closeness(y_true, y_hist40)))\n",
    "print('Exceeding: {} MW'.format(exceeding(y_true, y_hist40)))\n",
    "\n",
    "print('\\n')\n",
    "print('Max Exceeding: {} MW'.format(max_exceeding(y_true, y_hist40)))\n",
    "print('Average reserve ramp rate: {} MW/hr'.format(reserve_ramp_rate(y_true, y_hist40)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to fixed-width approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.9721415726499161%\n",
      "Requirement: 602.8499145507812 MW\n",
      "Closeness: 577.3825073242188 MW\n",
      "Exceeding: 188.73269653320312 MW\n",
      "\n",
      "\n",
      "Max Exceeding: 1582.3701171875 MW\n",
      "Average reserve ramp rate: 150.4645233154297 MW/hr\n"
     ]
    }
   ],
   "source": [
    "y_mean = np.mean(np.array([np.concatenate([d[(q2, CV)]['y_pred'] for CV in range(10)]), np.concatenate([d[(q1, CV)]['y_pred'] for CV in range(10)])]), axis = 0)\n",
    "y_fw = y_mean + 602.85 - np.mean(y_mean)\n",
    "\n",
    "print('Coverage: {}%'.format(coverage(y_true, y_fw)))\n",
    "print('Requirement: {} MW'.format(requirement(y_true, y_fw)))\n",
    "print('Closeness: {} MW'.format(closeness(y_true, y_fw)))\n",
    "print('Exceeding: {} MW'.format(exceeding(y_true, y_fw)))\n",
    "\n",
    "print('\\n')\n",
    "print('Max Exceeding: {} MW'.format(max_exceeding(y_true, y_fw)))\n",
    "print('Average reserve ramp rate: {} MW/hr'.format(reserve_ramp_rate(y_true, y_fw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[381.64902],\n",
       "       [422.73233],\n",
       "       [405.98123],\n",
       "       ...,\n",
       "       [466.72903],\n",
       "       [463.90845],\n",
       "       [461.76132]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_fw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NAbSZiaoJ4z"
   },
   "source": [
    "# 1. Data Ingress\n",
    "\n",
    "Load in net load forecast error data. Preprocessing of the data is handled by a separate package.\n",
    "\n",
    "TODO: Standardize the output for stability\n",
    "TODO: Add in the coverage percentage metric\n",
    "TODO: Visualization for model performance in different regimes.\n",
    "TODO: Identify unfinished business.\n",
    "\n",
    "TODO: Proper CV\n",
    "\n",
    "TODO: Basic Data check and Assertion\n",
    "\n",
    "Use `tf.data` to batch and shuffle the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hist40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI_percentiles = np.array([0.0125, 0.025, 0.975, 0.9875])\n",
    "model_directory = os.path.join(os.path.dirname(os.getcwd()),'trained_models','rescue_v1_0')\n",
    "\n",
    "batch_size = 32\n",
    "max_epochs = 20\n",
    "optimizer_choice = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in input and output of the training and validation samples from data pipeline\n",
    "input_trainval = pd.read_pickle(os.path.join('outputs_from_code','trainval_inputs.pkl'))\n",
    "output_trainval = pd.read_pickle(os.path.join('outputs_from_code','trainval_output.pkl'))\n",
    "n_samples = input_trainval.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "datetimes_trainval = pd.read_pickle(os.path.join('outputs_from_code','trainval_datetimes.pkl'))\n",
    "num_cv_folds = 10\n",
    "path_to_shuffled_indices = os.path.join(\"outputs_from_code\",\"day_block_shuffled_indices_v1_1.npy\")\n",
    "\n",
    "# Call function\n",
    "val_masks_all_folds = cross_val.get_CV_masks(datetimes_trainval, num_cv_folds, path_to_shuffled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_trainval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Definition\n",
    "Define the stucture of the model from start to finish. Includes normalization layer, ANN model and pinball loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class two_layer_ANN(tf.keras.Model):\n",
    "#     '''\n",
    "#     A two-layer ANN model with a pinball loss to produce prediction interval.\n",
    "#     The model can take an arbitrary amounts of input. As a rule of thumb less than 100 input terms would be preferred.\n",
    "#     The model's target/label should be the true value of the prediction whose upper/lower bound we are interested in. \n",
    "#     In our application, since we are interested in the range of forecast erros, the label should be the forecast error.\n",
    "    \n",
    "#     '''\n",
    "#     def __init__(self):\n",
    "#         super(two_layer_ANN, self).__init__()\n",
    "#         self.d1 = tf.keras.layers.Dense(10, activation='relu')\n",
    "#         self.d2 = tf.keras.layers.Dense(10, activation='relu')\n",
    "#         self.d3 = tf.keras.layers.Dense(1)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         x = self.d1(x)\n",
    "#         x = self.d2(x)\n",
    "#         x = self.d3(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the ANN model with Pinball loss using the Keras [functional API](https://www.tensorflow.org/guide/keras#model_subclassing)\n",
    "                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model that include the normalization layer\n",
    "inputs = tf.keras.Input(shape=input_trainval.shape[1:])\n",
    "\n",
    "# Create a Normalization layer and set its internal state using the training data\n",
    "normalizer = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "norm_inputs = normalizer(inputs)\n",
    "\n",
    "# A two-layer ANN network for regression\n",
    "dense1 = tf.keras.layers.Dense(10, activation='relu')\n",
    "hidden1 = dense1(norm_inputs)\n",
    "dense2 = tf.keras.layers.Dense(10, activation='relu')\n",
    "hidden2 = dense2(hidden1)\n",
    "dense3 = tf.keras.layers.Dense(1)\n",
    "outputs = dense3(hidden2)\n",
    "\n",
    "# define model from inputs to outputs\n",
    "rescue_model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PinballLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, tau=0.5, name=\"pinball_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.tau = tau\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        err = y_true - y_pred\n",
    "        skewed_mse = tf.math.reduce_mean(tf.math.maximum(self.tau * err, (self.tau - 1) * err), axis=0)\n",
    "\n",
    "        return skewed_mse\n",
    "\n",
    "\n",
    "class CoverageProbability(tf.keras.metrics.Metric):\n",
    "    \n",
    "    def __init__(self, name = 'CP', **kwargs):\n",
    "        super(CoverageProbability, self).__init__(name = name, **kwargs)\n",
    "        self.coverage_probability = self.add_weight(name = 'CP_Var', initializer=\"zeros\", dtype = tf.float64)\n",
    "        self.cum_n_samples = self.add_weight(name = 'n_samples', initializer=\"zeros\", dtype = tf.int32)\n",
    "        self.cum_n_covered = self.add_weight(name = 'n_covered', initializer=\"zeros\", dtype = tf.int32)\n",
    "        \n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # the state would be updated everytime we have a new calculation\n",
    "        self.cum_n_samples.assign_add(tf.size(y_pred, out_type=tf.int32))\n",
    "        self.cum_n_covered.assign_add(tf.math.count_nonzero(tf.math.less_equal(y_true,y_pred), dtype = tf.int32))\n",
    "        self.coverage_probability.assign(tf.math.divide(self.cum_n_covered, self.cum_n_samples))\n",
    "\n",
    "    def result(self):\n",
    "        return self.coverage_probability\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.coverage_probability.assign(0.0)\n",
    "        \n",
    "        \n",
    "class AverageIntervalWidth(tf.keras.metrics.Metric):\n",
    "    \n",
    "    def __init__(self, name = 'AIW', **kwargs):\n",
    "        \n",
    "        super(AverageIntervalWidth, self).__init__(name = name, **kwargs)\n",
    "        self.average_interval_width = self.add_weight(name = 'AIW', initializer=\"zeros\", dtype = tf.float32)\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # the state would be updated everytime we have a new calculation\n",
    "        self.average_interval_width.assign(tf.math.reduce_mean(y_pred))\n",
    "\n",
    "    def result(self):\n",
    "        return self.average_interval_width\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.average_interval_width.assign(0.0)\n",
    "\n",
    "\n",
    "# early stopping criteria is validation loss. \n",
    "# Min loss change of 1 in 2 epochs (patience)\n",
    "cb_early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=1, patience=2 , verbose=1)\n",
    "\n",
    "# we will overwrite the current checkpoint if and only if the `val_loss` score has improved.\n",
    "# The saved model name will include the current epoch. and into the model_directory folder\n",
    "def get_cb_check_points_for_tau(model_directory, tau):\n",
    "    # make sure models for different tau go to different directories\n",
    "    ckpt_directory = os.path.join(model_directory, \"tau={:.0%}\".format(tau))\n",
    "    if not os.path.exists(ckpt_directory):\n",
    "        os.makedirs(ckpt_directory)\n",
    "    cb_check_points = tf.keras.callbacks.ModelCheckpoint(filepath=ckpt_directory, save_best_only=True, monitor=\"val_loss\", verbose=0)\n",
    "    return cb_check_points\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rescue_model_set = {}\n",
    "data = {}\n",
    "history = {}\n",
    "\n",
    "for tau in PI_percentiles:\n",
    "    print(\"Training model for Prediction interval: {:.0%}\".format(tau))\n",
    "    for fold_idx in range(num_cv_folds):\n",
    "        print(\"Cross Validation fold #\", fold_idx+1)\n",
    "        \n",
    "        # Split into training and validation dataset based on the validation masks of this CV fold\n",
    "        input_train, output_train = input_trainval[~val_masks_all_folds[fold_idx]], output_trainval[~val_masks_all_folds[fold_idx]]\n",
    "        input_val, output_val = input_trainval[val_masks_all_folds[fold_idx]], output_trainval[val_masks_all_folds[fold_idx]]\n",
    "        \n",
    "        # cast to 'float32'\n",
    "        input_train = input_train.values.astype('float32')\n",
    "        output_train = output_train.values.astype('float32')\n",
    "        input_val = input_val.values.astype('float32')\n",
    "        output_val = output_val.values.astype('float32')\n",
    "\n",
    "        # Transoform into tensorflow Dataset type for training\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((input_train, output_train)).shuffle(buffer_size= n_samples).batch(batch_size)\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((input_val, output_val)).shuffle(buffer_size = n_samples).batch(batch_size)\n",
    "\n",
    "\n",
    "        rescue_model_set[(tau, fold_idx)] = tf.keras.models.clone_model(rescue_model)\n",
    "        rescue_model_set[(tau, fold_idx)].get_layer(index = 1).adapt(input_trainval.values) \n",
    "        rescue_model_set[(tau, fold_idx)].compile(loss = PinballLoss(tau = tau), \n",
    "                                                  metrics= [CoverageProbability(), \n",
    "                                                            AverageIntervalWidth()],\n",
    "                                                  optimizer=optimizer_choice)\n",
    "        \n",
    "        history[(tau, fold_idx)]= rescue_model_set[(tau, fold_idx)].fit(train_ds, validation_data=val_ds, epochs=max_epochs,\n",
    "                                                                        callbacks=[cb_early_stopping, get_cb_check_points_for_tau(model_directory, tau)])\n",
    "        \n",
    "        data[(tau, fold_idx)] = {}\n",
    "        data[(tau, fold_idx)]['y_pred'] = rescue_model_set[(tau, fold_idx)].predict(input_val)\n",
    "        data[(tau, fold_idx)]['y_true'] = output_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data)\n",
    "df.to_pickle(os.path.join('outputs_from_code', 'metrics_data_v2.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_idx in range(num_cv_folds):\n",
    "    \n",
    "    print(sum(predictions[(0.25, fold_idx)].squeeze() > predictions[(0.5, fold_idx)].squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history[(tau, fold_idx)].history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing cross-validation performance and generalizability of the model\n",
    "# reassemble the best performing model's loss\n",
    "train_losses = pd.DataFrame(None, index = PI_percentiles)\n",
    "val_losses = train_losses.copy()\n",
    "train_cp = train_losses.copy()\n",
    "val_cp = train_losses.copy()\n",
    "\n",
    "for tau in PI_percentiles:\n",
    "    for fold_idx in range(num_cv_folds):\n",
    "        train_losses.loc[tau, fold_idx] = history[(tau, fold_idx)].history['loss'][-1]\n",
    "        train_cp.loc[tau,fold_idx] = history[(tau, fold_idx)].history['CP'][-1]\n",
    "        val_losses.loc[tau, fold_idx] = history[(tau, fold_idx)].history['val_loss'][-1]\n",
    "        val_cp.loc[tau,fold_idx] = history[(tau, fold_idx)].history['val_CP'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the training process\n",
    "\n",
    "fig, axarr = plt.subplots(1,2, sharex = True)\n",
    "train_x_pos =np.expand_dims(np.array(PI_percentiles),1)*np.ones((1,10)) - 0.03\n",
    "val_x_pos = np.expand_dims(np.array(PI_percentiles),1)*np.ones((1,10)) + 0.03\n",
    "\n",
    "axarr[0].scatter(train_x_pos.ravel(), train_losses.values.ravel(),label = 'Training', alpha = 0.5)\n",
    "axarr[0].scatter(val_x_pos.ravel(), val_losses.values.ravel(), label = 'Validation',alpha = 0.5)\n",
    "axarr[0].set_xticks(PI_percentiles)\n",
    "axarr[0].set_xlabel('Tau (Target Percentile) (%)')\n",
    "axarr[0].set_ylabel('Losses (MW)')\n",
    "\n",
    "axarr[1].scatter(train_x_pos.ravel(), train_cp.values.ravel(), label = 'Training' ,alpha = 0.5)\n",
    "axarr[1].scatter(val_x_pos.ravel(), val_cp.values.ravel(), label = 'Validation',alpha = 0.5)\n",
    "axarr[1].set_xticks(PI_percentiles)\n",
    "axarr[1].set_xlabel('Tau (Target Percentile) (%)')\n",
    "axarr[1].set_ylabel('Coverage Probability (%)')\n",
    "\n",
    "axarr[1].legend(loc = 'center left', bbox_to_anchor = [1,0.5], frameon = False)\n",
    "fig.set_size_inches(10,4)\n",
    "fig.tight_layout()\n",
    "fig.savefig('cross_validation_and_generalizability.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(1,2, sharex = True)\n",
    "history_eg = history[(0.25,0)].history\n",
    "\n",
    "axarr[0].plot(history_eg['loss'],label = 'Training')\n",
    "axarr[0].plot(history_eg['val_loss'], label = 'Validation')\n",
    "\n",
    "axarr[0].set_xlabel('Epochs (#)')\n",
    "axarr[0].set_ylabel('Losses (MW)')\n",
    "\n",
    "axarr[1].plot(history_eg['CP'], label = 'Training')\n",
    "axarr[1].plot(history_eg['val_CP'], label = 'Validation')\n",
    "axarr[1].axhline(0.25, label = 'Target CP', dashes = [2,2], color = 'k')\n",
    "axarr[1].set_xlabel('Epochs (#)')\n",
    "axarr[1].set_ylabel('Coverage Probability (%)')\n",
    "\n",
    "axarr[1].legend(loc = 'center left', bbox_to_anchor = [1,0.5], frameon = False)\n",
    "fig.set_size_inches(10,4)\n",
    "fig.tight_layout()\n",
    "fig.savefig('training_history.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_timeseries = pd.DataFrame(index = pd.to_datetime(datetimes_trainval.loc['T+1'], infer_datetime_format=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing forecast uncertainty over different times of the day\n",
    "low_band_forecast = rescue_model_set[(0.25,0)].predict(input_trainval.values)\n",
    "median_forecast = rescue_model_set[(0.5,0)].predict(input_trainval.values)\n",
    "high_band_forecast = rescue_model_set[(0.75,0)].predict(input_trainval.values)\n",
    "\n",
    "prediction_timeseries['low'] = np.squeeze(low_band_forecast)\n",
    "prediction_timeseries['median'] = np.squeeze(median_forecast)\n",
    "prediction_timeseries['high'] = np.squeeze(high_band_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diurnal_trend = prediction_timeseries.groupby(prediction_timeseries.index.hour).mean()\n",
    "\n",
    "diurnal_trend.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diurnal_trend_debiased = diurnal_trend - np.expand_dims(diurnal_trend['median'].values, axis=1)\n",
    "diurnal_trend_debiased.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes_trainval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_range = np.arange(5000,5192)\n",
    "input_val_eg = input_val[example_range]\n",
    "output_val_eg = output_val[example_range]\n",
    "\n",
    "# confirm the PI percentiles are symmetrical\n",
    "for PI in PI_percentiles:\n",
    "    assert 1-PI in PI_percentiles, \"Not all PI intervals are constructed symmetrically!\"\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(example_range, output_val_eg, color = 'C1', label = 'True forecast error')\n",
    "\n",
    "# plot median forecast if it is being produced\n",
    "if 0.5 in PI_percentiles:\n",
    "    ax.plot(example_range, rescue_model_set[0.5].predict(input_val_eg), color = 'C0', label = 'PI - 50%')\n",
    "\n",
    "for PI in PI_percentiles:\n",
    "    if PI<0.5:\n",
    "        ax.fill_between(example_range, np.squeeze(rescue_model_set[PI].predict(input_val_eg)),\n",
    "                        np.squeeze(rescue_model_set[1-PI].predict(input_val_eg)),\n",
    "                        color = 'C0', alpha = PI, label = '{:.0%}-{:.0%} range'.format(PI, 1-PI))\n",
    "\n",
    "ax.set_ylabel(\"net load forecast error (MW)\")\n",
    "ax.legend()\n",
    "fig.set_size_inches(12,4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "inherent_bias_eg = np.squeeze(rescue_model_set[0.5].predict(input_val[example_range]))\n",
    "\n",
    "ax.plot(example_range, np.squeeze(output_val_eg) - inherent_bias_eg, label = 'Forecast error (de-biased)')\n",
    "\n",
    "for PI in PI_percentiles:\n",
    "    if PI<0.5:\n",
    "        ax.fill_between(example_range, np.squeeze(rescue_model_set[PI].predict(input_val_eg)) - inherent_bias_eg,\n",
    "                        np.squeeze(rescue_model_set[1-PI].predict(input_val_eg)) - inherent_bias_eg,\n",
    "                        color = 'C0', alpha =PI, label = '{:.0%}-{:.0%} range'.format(PI, 1-PI))\n",
    "\n",
    "\n",
    "ax.set_ylabel(\"Net load forecast error, De-biased (MW)\")\n",
    "ax.legend()\n",
    "fig.set_size_inches(12,4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGih-c2LgbJu"
   },
   "source": [
    "Choose an optimizer and loss function for training: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JB6A1vcigsIe"
   },
   "source": [
    "Select metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over epochs and then print the overall result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0MqHFb4F_qn"
   },
   "outputs": [],
   "source": [
    "# train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "# test_loss = tf.keras.metrics.Mean(name='test_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ix4mEL65on-w"
   },
   "source": [
    "Use `tf.GradientTape` to train the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8YT7UmFgpjV"
   },
   "source": [
    "Test the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIKdEzHAJGt7"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, outputs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(outputs, predictions)\n",
    "    gradients = tape.gradient(loss, rescue.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, rescue.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    #train_accuracy(outputs, predictions)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(inputs, outputs, model, loss_fn):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = rescue(inputs, training=False)\n",
    "    t_loss = loss_object(outputs, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    #test_accuracy(outputs, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-2pkctU_Ci7"
   },
   "outputs": [],
   "source": [
    "# EPOCHS = 5\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#   # Reset the metrics at the start of the next epoch\n",
    "#   train_loss.reset_states()\n",
    "#   #train_accuracy.reset_states()\n",
    "#   test_loss.reset_states()\n",
    "#   #test_accuracy.reset_states()\n",
    "\n",
    "#   for inputs, outputs in train_ds:\n",
    "#     train_step(inputs, outputs)\n",
    "\n",
    "#   for test_inputs, test_outputs in test_ds:\n",
    "#     test_step(test_inputs, test_outputs)\n",
    "\n",
    "#   print(\n",
    "#     f'Epoch {epoch + 1}, '\n",
    "#     f'Loss: {train_loss.result()}, '\n",
    "#     #f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "#     f'Test Loss: {test_loss.result()}, '\n",
    "#     #f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "advanced.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
