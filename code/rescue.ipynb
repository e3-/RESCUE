{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOsVdx6GGHmU"
   },
   "source": [
    "# RESCUE\n",
    "**Renewable Energy Salient Combined Uncertainty Estimator**\n",
    "\n",
    "## *Description:*\n",
    "A machine-learning based framework to quantify the short-term uncertainty in netload forecast developed by E3. \n",
    "The main structure of the model include a multi-layer artificial neural net with the pinball loss function as objective. Conditional on combinations of input, the model is able to output quantile forecast for the net-load forecast error.\n",
    "\n",
    "This notebook contains the work stream of ingesting pre-processed data, set up cross validation folds, training and deployment, and calling functions for diagnostics. For detailed implementation of data preprocessing or quoted functions, please refer to other script files. This project is available in the [e3/RESCUE](https://github.com/e3-/RESCUE) Github online repository. \n",
    "\n",
    "RESCUE model supports multi-objective learning. For example, in addition to producing the quantile forecast of Net Load forecast error, it can be trained to simultaneously predict the Load, Solar and Wind forecast error. The objectives can be weighted per user's judgement of relative importance.\n",
    "\n",
    "Special note on CAISO use case: In other cases, the quantile forecast is trained on forecast errors, and represent the probablity distribution of forecast errors. For CAISO, as we are using RTPD forecast - RTD forecast as the response variable, it represents the distribution on forecast difference rather than forecast error. Nevertheless, the model structure and the logic still holds the same. \n",
    "## *Highlights:*\n",
    "1. Incorporating a wide gamut of information: weather, calendar, forecast, and lagged error aware. \n",
    "2. Intrinsically handles resource correlation as solar,wind, and load errors are co-trained within the model.\n",
    "3. Produces multiple prediction intervals for expected error in netload, load, solar and wind forecasting, for cherry picking down-stream\n",
    "4. Model agnostic. No requirement on knowledge of the inner workings of the netload forecast\n",
    "5. Adheres to best practice in statistics: cross validation, normalization, early-stopping, etc.\n",
    "\n",
    "## *To-dos:*\n",
    "1. Standardize the output for stability;\n",
    "3. Make actual reserves data part of dir_str if we foresee comparing model predictions to actual reserves in future work\n",
    "\n",
    "## *Authors:* \n",
    "Yuchi Sun, Vignesh Venugopal, Charles Gulian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0trJmd6DjqBZ"
   },
   "outputs": [],
   "source": [
    "# Import third party packages\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import functools\n",
    "\n",
    "# Import self defined packages\n",
    "import cross_val\n",
    "import utility\n",
    "import diagnostics\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. User Inputs\n",
    "## 0.1 Model Training and Inference Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the model to either be trained from scratch or to be restored for diagnostics. Recommend to include version number\n",
    "model_name = 'rescue_v1_3'\n",
    "\n",
    "# Target quantiles of the prediction \n",
    "PI_percentiles = np.array([0.025, 0.05, 0.25, 0.5,0.75, 0.95, 0.975]) # quantiles to predict\n",
    "\n",
    "# Structural parameter of the ANN network\n",
    "num_neurons = 10\n",
    "activation_type = 'relu'\n",
    "\n",
    "# Relative importance of model outputs. Order MUST match order of output variables\n",
    "loss_wts = [np.sqrt(3), 1.0, 1.0, 1.0]\n",
    "\n",
    "# Cross validation parameters\n",
    "num_cv_folds = 10 # number of cross validation folds\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64 # size of each mini-batch in the SGD\n",
    "max_epochs = 50 # Maximum number of epochs in training. In each epoch, each training data is used exactly once\n",
    "optimizer_choice = 'adam' # Optimizer choice. Default to ADAM, a popular choice that have 1st and 2nd order momentum\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stop_monitor = 'val_loss' # The metrics to watch when deciding whether to stop. Recommendation: Validation loss as in 'val_loss'\n",
    "early_stop_min_delta = 0.5 # if the difference/decrease in loss is less than min_delta, the model is considered no longer improving\n",
    "early_stop_patience = 3 # For number of patience epochs, observe if the model has improved more than min_delta\n",
    "early_stop_verbosity = 1 # 0: no output, 1: some output, 2: full output\n",
    "\n",
    "# Check points parameters\n",
    "ckpt_monitor = 'val_loss' # Check points are only saved when there is an improvement in ckpt monitor\n",
    "\n",
    "# Losses and Metrics log parameters\n",
    "log_activation_freq = 0 # the frequency of logging hidden layer activation's histogram. Default to not log\n",
    "log_update_freq = 'epoch' # The frequency of logging. Default to record at the end of every epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Diagnostics and Visualization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization parameters\n",
    "eg_fold_for_visualization = 0 # the example fold of the trained models to use for plotting model output\n",
    "default_dpi = 300 # dpi of the output graphics\n",
    "\n",
    "# Define the mapping between internal feature (model input) name and feature label used in plotting\n",
    "label_to_feature_map = {\"Solar_RTPD_Forecast_T+1\":\"Solar Generation (MW)\",\n",
    "                       \"Wind_RTPD_Forecast_T+1\":\"Wind Generation (MW)\",\n",
    "                       \"Load_RTPD_Forecast_T+1\":\"Load (MW)\",\n",
    "                       \"Days_from_Start_Date_T+1\":\"Date of Observation\",\n",
    "                       \"Hour_Angle_T+1\":\"Hour\",\n",
    "                       \"Day_Angle_T+1\":\"Month\"}\n",
    "\n",
    "# Define the mapping between internal response (model output) name and response label used in plotting\n",
    "label_to_response_map = {\"Net_Load_Forecast_Error_T+1\":\"Net Load\",\n",
    "                       \"Load_Forecast_Error_T+1\":\"Load\",\n",
    "                       \"Solar_Forecast_Error_T+1\":\"Solar\",\n",
    "                       \"Wind_Forecast_Error_T+1\":\"Wind\"}\n",
    "\n",
    "# Define the mapping between internal response (model output) name and its type\n",
    "# Type MUST be one of \"load\" or \"generation\". Used to determine sign-convention while plotting\n",
    "response_type_map = pd.Series({\"Net_Load_Forecast_Error_T+1\":\"load\",\n",
    "                               \"Load_Forecast_Error_T+1\":\"load\",\n",
    "                               \"Solar_Forecast_Error_T+1\":\"generation\",\n",
    "                               \"Wind_Forecast_Error_T+1\":\"generation\"})\n",
    "\n",
    "# Training and validation loss comparison for multiple folds\n",
    "x_jitter = 0.1\n",
    "metrics_to_idx_map = {'Loss (MW)':0, 'Coverage Probability (%)':1}\n",
    "metrics_to_compare = ['Loss (MW)', 'Coverage Probability (%)'] # choose in metrics_to_idx_map's keys\n",
    "\n",
    "# Time series\n",
    "ts_ranges = ['20170201','20170501','20170801','20171101'] # the example range of time series to plot\n",
    "\n",
    "#TODO: Cycle for all years and all months\n",
    "\n",
    "# Set this variable to false if not plotting a comparison to other reserves methods\n",
    "is_plotting_comparative_methods = True\n",
    "# a dictionary that has the method's name as key. and the method's reserve path as value\n",
    "# Files MUST have an UP and a DOWN column, holding headroom and footroom in MW, both of which MUST be positive\n",
    "s_drive_rescue_data = r\"S:\\E3 Projects\\ARPA_E _PERFORM\\Task_3_Machine_Learning\\RESCUE\\data\\raw_data\"\n",
    "comparative_reserves_path = {\"Histogram\": os.path.join(s_drive_rescue_data, \n",
    "                                                       \"Flex_Ramp_Req_RTPD_Histogram.csv\"),\n",
    "                             \"Quant Reg\": os.path.join(s_drive_rescue_data, \n",
    "                                                       \"Flex_Ramp_Req_RTPD_Quantile_Reg.csv\")}\n",
    "\n",
    "# Quantiles used in the monthly coincendent comparison between historical and prediction. \n",
    "# As a suggestion should be set at the percentiles where headroom/footrooms are set\n",
    "coincident_comp_quantiles = [0.025, 0.975]\n",
    "\n",
    "## Pareto Front comparison\n",
    "# models to compare in the pareto front comparison section\n",
    "models_to_compare = ['rescue_v1_1_multi_objective', 'rescue_v1_3'] # List of models to compare; must be multi-objective models\n",
    "output_for_pareto_comp = \"Net_Load_Forecast_Error_T+1\" # the specific output and its metrics we would focus on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NAbSZiaoJ4z"
   },
   "source": [
    "# 1. Data Ingestion\n",
    "\n",
    "Load in pre-processed trainval dataset that includes all input features/ output response(s) for both training and validation. Prepare the cross validation splitting masks. Set up directory structure to store intermediate (`log`, `checkpoints`) and final outputs (`models`, `outputs`,`diagnostics`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day block shuffling pre-determnined....\n",
      "Done....\n",
      "Creating train val masks for each fold....\n",
      "Train and val masks are ready!\n"
     ]
    }
   ],
   "source": [
    "# Load in/ Create folder structure for the current model\n",
    "dir_str = utility.Dir_Structure(model_name = model_name)\n",
    "\n",
    "# Read in input and output of the training and validation samples from data pipeline. \n",
    "# This should be an output of the data_preprocessing script\n",
    "input_trainval = pd.read_pickle(dir_str.input_trainval_path)\n",
    "output_trainval = pd.read_pickle(dir_str.output_trainval_path)\n",
    "\n",
    "num_samples = input_trainval.shape[0]\n",
    "num_outputs = output_trainval.shape[1]\n",
    "\n",
    "# Use cross validation script to conduct intra-day consecutive trainval splitting. The number of folds is \n",
    "# determined by num_cv_folds. The data of the same day would not end up separately in training and validation\n",
    "# to not overestimate model performance.\n",
    "val_masks_all_folds = cross_val.get_CV_masks(input_trainval.index, num_cv_folds, dir_str.shuffled_indices_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct basic data validation to avoid some of the most common data pitfalls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since historical data quatiles will be compared to model predictions, ensure model predictions exist at these exact quantiles\n",
    "assert set(coincident_comp_quantiles)<= set(PI_percentiles)\n",
    "\n",
    "# confirm the PI percentiles are symmetrical and 0.5 is one of the target quantile\n",
    "for PI in PI_percentiles:\n",
    "    assert np.allclose(1-PI_percentiles, PI_percentiles[::-1]), \"Not all PI intervals are constructed symmetrically!\"\n",
    "assert 0.5 in PI_percentiles, \"Median forecast (P50) must be produced!\"\n",
    "\n",
    "# Make sure input and output shape match\n",
    "assert input_trainval.shape[0] == output_trainval.shape[0], \"Input and output shape mismatch!\"\n",
    "\n",
    "# Make sure output and output weights shape match\n",
    "assert output_trainval.shape[1] == len(loss_wts), \"Number of 'loss_wts' must equal number of outputs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Definition\n",
    "Define the various component of the model that are active at various stage of the model life cycle. Before training: model structure, loss function. During training: call backs and metrics. After training: Saving functions. \n",
    "\n",
    "## 2.1 Model Structure.\n",
    "The RESCUE model is built with the Keras [functional API](https://www.tensorflow.org/guide/keras#model_subclassing). As it stands right now, it is a two layer ANN network with a pre-processing normalization layer. Rectified linear units (RELU) is used as the activation function for the hidden layers, while the last layer is a direct linear regression.                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model that include the normalization layer\n",
    "inputs = tf.keras.Input(shape=input_trainval.shape[1:])\n",
    "\n",
    "# Create a Normalization layer and set its internal state using the training data\n",
    "normalizer = tf.keras.layers.experimental.preprocessing.Normalization(name = 'Normalization')\n",
    "norm_inputs = normalizer(inputs)\n",
    "\n",
    "# A two-layer ANN network for regression\n",
    "dense1 = tf.keras.layers.Dense(num_neurons, activation=activation_type)\n",
    "hidden1 = dense1(norm_inputs)\n",
    "dense2 = tf.keras.layers.Dense(num_neurons, activation=activation_type)\n",
    "hidden2 = dense2(hidden1)\n",
    "dense3 = tf.keras.layers.Dense(num_outputs)\n",
    "outputs = dense3(hidden2)\n",
    "\n",
    "# define model from inputs to outputs\n",
    "rescue_model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Loss Function\n",
    "\n",
    "The pinball loss function is used here to provide a quantile forecast rather than a median forecast. With increasing sample size, the pinball loss would converge to the quantile forecast of a variable conditional on the input variable. The quantile is given by a parameter `tau`, which is set in the user input as PI percentiles. \n",
    "\n",
    "One property of the pinball loss is that for 0% and 100% percentile, the pinball loss is always 0 no matter the model and parameter choice, while the median forecast have the highest loss. So comparing the losses across different quantiles makes little sense, and is advised against. For more information on pinball losses, check out [this wiki](https://en.wikipedia.org/wiki/Quantile_regression).\n",
    "\n",
    "In case of multi-output, multi-objective regression, the loss corresponding to each output can be weighted based on the importance given to each output. The loss that the model will ultimately try to minimize would be the weigted average pinball loss of all the outputs. In the specific and common case of net load co-trained with wind,solar, and load, the recommended weight raio is $\\sqrt{3}$:1:1:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PinballLoss(tf.keras.losses.Loss):\n",
    "    \n",
    "    def __init__(self, tau=0.5, loss_wts=[1], name=\"pinball_loss\", **kwargs):\n",
    "        super().__init__(name=name)\n",
    "        self.tau = tau # the target quantile\n",
    "        self.loss_wts = loss_wts # Relative importance of loss corresponding to each output\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        err = y_true - y_pred # the convention is always true - pred\n",
    "        # essentially, quantile regression takes two region. For values bigger than the quantile forecast,\n",
    "        # they are weighted by 1-tau, while for values smaller than the forecast it's weighted by tau.\n",
    "        loss_for_each_output_var_of_each_sample = tf.math.maximum(self.tau * err, (self.tau - 1) * err)\n",
    "        # Get weighted avg loss for each sample\n",
    "        loss_for_each_sample = tf.math.reduce_mean(tf.math.multiply(loss_for_each_output_var_of_each_sample, self.loss_wts),\n",
    "                                                   axis = 1)\n",
    "        # Now get a single loss across all samples that make up the batch\n",
    "        skewed_mse = tf.math.reduce_mean(loss_for_each_sample, axis=0)\n",
    "\n",
    "        return skewed_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Metrics\n",
    "\n",
    "In tensorflow terminology, metrics are quantities that are calculated as the training goes on to aid your judgement on model's fitness and completeness. In our use cases, we define two metrics for this purpose: coverage probability and average interval width.\n",
    "\n",
    "For prediction intervals, **coverage probability** refers to how often are the actual forecast included in the prediciton interval bands. Here we slightly modify the definition and refers to how often are the actual forecast smaller than the target quantile. For a well behaving model, the CP would converge to target quantile tau.\n",
    "\n",
    "For **average interval width**, it normally refers to the width of a prediction interval band. Again, we make a slight modification here. Since in practice, the forecast for quantiles above 50% are upwards reserve and normally positive, and below 50% are downwards reserve and normally negative, we are simply using the quantile forecast's absolute distance to 0 as the interval width here. For interpretation, we are looking for smaller requirement for better band width, but also high flexibitliy wrt to varying condidtions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoverageProbability(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name = 'CP',**kwargs):\n",
    "        super(CoverageProbability, self).__init__(name = name, **kwargs)\n",
    "        self.coverage_probability = self.add_weight(name = 'CP', initializer=\"zeros\", dtype = tf.float64)\n",
    "        # the cumulative number of samples and the number of samples smaller than current quantile forecast\n",
    "        self.cum_n_samples = self.add_weight(name = 'n_samples', initializer=\"zeros\", dtype = tf.int32)\n",
    "        self.cum_n_covered = self.add_weight(name = 'n_covered', initializer=\"zeros\", dtype = tf.int32)\n",
    "        \n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # the state would be updated everytime we have a new calculation\n",
    "        self.cum_n_samples.assign_add(tf.size(y_pred, out_type=tf.int32))\n",
    "        self.cum_n_covered.assign_add(tf.math.count_nonzero(tf.math.less_equal(y_true,y_pred), dtype = tf.int32))\n",
    "        # cp = n_covered/n_samples\n",
    "        self.coverage_probability.assign(tf.math.divide(self.cum_n_covered, self.cum_n_samples))\n",
    "\n",
    "    def result(self):\n",
    "        return self.coverage_probability\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.coverage_probability.assign(0.0)\n",
    "        \n",
    "class AverageIntervalWidth(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='AIW', **kwargs):\n",
    "        super(AverageIntervalWidth, self).__init__(name=name, **kwargs)\n",
    "        self.average_interval_width = self.add_weight(name='AIW', initializer=\"zeros\", dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # the state would be updated everytime we have a new calculation\n",
    "        self.average_interval_width.assign(tf.math.reduce_mean(y_pred))\n",
    "\n",
    "    def result(self):\n",
    "        return self.average_interval_width\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.average_interval_width.assign(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Callbacks\n",
    "\n",
    "In tensorflow terminology, callbacks are functions that get executed with certain frequency during training. For our purpose, all the callbacks happen once per epoch and we are using three types of callbacks: Early stopping, check points, and tensor boards. Saving the model is not necessarily a callback, but it also get executed once after the training for the model is complete, so is included in this segment.\n",
    "\n",
    "1. Early stopping stops the training when certain criteria is met. In general the criteria is that when `monitor` did not improve by more than `min_delta` in `patience` epoch(s), then the training is stopped. This is a way to effectively prevent overfitting to the training data, so the monitor is most often validation loss.  \n",
    "2. Check points are periodical snap shot of parameter weights saved in case the model training is unexpectedly stopped.  We are saving the `checkpoints`,`logs`, and`trained models` in different folders for different taus and folds.\n",
    "3. Saving is very similar to check points. Difference is that check points are conducted at the end of every epoch, while saving only happens at the end of training session.\n",
    "4. The tensorboard callback allow us to visualize and observe losses and all metrics in a pre-compiled tensorboard interface. It can even visualize losses and metrics for multiple folds and taus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping stops the training when certain criteria is met.\n",
    "cb_early_stopping = tf.keras.callbacks.EarlyStopping(monitor=early_stop_monitor, min_delta=early_stop_min_delta, \n",
    "                                                     patience=early_stop_patience, verbose=early_stop_verbosity)\n",
    "\n",
    "# For the save best only parameter: we will overwrite the current checkpoint if and only if the `val_loss` \n",
    "# score has improved. Different fold and tau would end up in different ckpts_dir folder\n",
    "def get_cb_check_points(tau, fold_idx):\n",
    "    # make sure models for different tau go to different directories\n",
    "    ckpts_dir = os.path.join(dir_str.ckpts_dir, \"tau_{:.1%}\".format(tau),\"fold_#{}\".format(fold_idx))\n",
    "    if not os.path.exists(ckpts_dir):\n",
    "        os.makedirs(ckpts_dir)\n",
    "    cb_check_points = tf.keras.callbacks.ModelCheckpoint(filepath=ckpts_dir, save_best_only=True, monitor= ckpt_monitor, verbose=0)\n",
    "    return cb_check_points\n",
    "\n",
    "\n",
    "# Currrently not logging the histogram of activation and embedding layers. Write log per epoch.\n",
    "def get_cb_tensor_board(tau, fold_idx):\n",
    "    # make sure models for different tau and fold would get logged in different directory\n",
    "    logs_dir = os.path.join(dir_str.logs_dir, \"tau_{:.1%}\".format(tau),\"fold_#{}\".format(fold_idx))\n",
    "    if not os.path.exists(logs_dir):\n",
    "        os.makedirs(logs_dir)\n",
    "    cb_tensor_board = tf.keras.callbacks.TensorBoard(logs_dir, histogram_freq= log_activation_freq, \n",
    "                                                     embeddings_freq=0,  update_freq=log_update_freq)  \n",
    "    return cb_tensor_board\n",
    "\n",
    "# Save the model by the end of each training session. Might be replacible by checkpoints.\n",
    "def save_rescue_model(model, tau, fold_idx):\n",
    "    # make sure models for different tau and fold would get logged in different directory\n",
    "    models_dir = os.path.join(dir_str.models_dir, \"tau_{:.1%}\".format(tau),\"fold_#{}\".format(fold_idx))\n",
    "    if not os.path.exists(models_dir):\n",
    "        os.makedirs(models_dir)\n",
    "    \n",
    "    model.save(models_dir)\n",
    "    return None   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Training\n",
    "\n",
    "Training is the process where the parameter of a model changes to reduce some loss function. In our specific case, we are conducting training separtely for each target quantile and each fold. We first split data into training and validation based on current fold number, and then initialize a new model to fit to the training data until the pinball loss meet some stopping criteria, i.e. showing no significant decrease.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for Prediction interval: 2.5%\n",
      "Cross Validation fold # 1\n",
      "Epoch 1/50\n",
      "1068/1400 [=====================>........] - ETA: 0s - loss: 74.8342 - CP: 0.3446 - AIW: -178.4248"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-72e15f78b085>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# The training process. Passing in callbacks to use in mid training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         history[(tau, fold_idx)]= rescue_model_set[(tau, fold_idx)].fit(train_ds, validation_data=val_ds, epochs=max_epochs,\n\u001b[1;32m---> 35\u001b[1;33m                                                                         callbacks=[cb_early_stopping, get_cb_tensor_board(tau, fold_idx)])\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m# Save the trained rescue model for each target percentile and fold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rescue_model_set = {} # intialized container for TF models. Indexed by (tau, fold_idx)\n",
    "history = {} # intialized container for the training history of models. Indexed by (tau, fold_idx)\n",
    "\n",
    "# loop through different target quantiles and cross validation folds\n",
    "for tau in PI_percentiles:\n",
    "    print(\"Training model for Prediction interval: {:.1%}\".format(tau))\n",
    "    for fold_idx in range(num_cv_folds):\n",
    "        print(\"Cross Validation fold #\", fold_idx+1)\n",
    "        \n",
    "        # Split into training and validation dataset based on the CV validation masks generated in section 1.\n",
    "        input_train, output_train = input_trainval[~val_masks_all_folds[fold_idx]], output_trainval[~val_masks_all_folds[fold_idx]]\n",
    "        input_val, output_val = input_trainval[val_masks_all_folds[fold_idx]], output_trainval[val_masks_all_folds[fold_idx]]\n",
    "        \n",
    "        # retain value only and cast to 'float32'. Single precision calculate a lot faster than double precision.\n",
    "        input_train = input_train.values.astype('float32')\n",
    "        output_train = output_train.values.astype('float32')\n",
    "        input_val = input_val.values.astype('float32')\n",
    "        output_val = output_val.values.astype('float32')\n",
    "\n",
    "        # Using tf.data API to batch and shuffle the dataset. For shuffling, the buffer size should be bigger than the total \n",
    "        # sample count. Or else only the first buffle size of samples would be shuffled \n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((input_train, output_train)).shuffle(buffer_size= num_samples).batch(batch_size)\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((input_val, output_val)).shuffle(buffer_size = num_samples).batch(batch_size)\n",
    "\n",
    "        # Make a fresh clone of the rescue model for the specific quantile and fold\n",
    "        rescue_model_set[(tau, fold_idx)] = tf.keras.models.clone_model(rescue_model)\n",
    "        # For some layers the paramers are not trainable, and is adapted at the begining to data. E.g.:Normalization layer \n",
    "        rescue_model_set[(tau, fold_idx)].get_layer('Normalization').adapt(input_trainval.values)\n",
    "        # Compiling the loss, optimizer, metrics, and model into one compiled instance\n",
    "        rescue_model_set[(tau, fold_idx)].compile(loss=PinballLoss(tau=tau, loss_wts=loss_wts),\n",
    "                                                  optimizer=optimizer_choice, metrics=[CoverageProbability(),AverageIntervalWidth()])\n",
    "        \n",
    "        # The training process. Passing in callbacks to use in mid training \n",
    "        history[(tau, fold_idx)]= rescue_model_set[(tau, fold_idx)].fit(train_ds, validation_data=val_ds, epochs=max_epochs,\n",
    "                                                                        callbacks=[cb_early_stopping, get_cb_tensor_board(tau, fold_idx)])\n",
    "        \n",
    "        # Save the trained rescue model for each target percentile and fold\n",
    "        save_rescue_model(rescue_model_set[(tau, fold_idx)], tau, fold_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Re-loading trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Load models\n",
    "# rescue_model_set = {}\n",
    "\n",
    "# # Initialize container for the inference result for different quantiles/CV folds\n",
    "# multi_index_tau_folds = pd.MultiIndex.from_product([PI_percentiles, range(num_cv_folds), output_trainval.columns.values],\n",
    "#                                                    names = ['Quantiles','Fold ID', \"Output_Name\"])\n",
    "# pred_trainval = pd.DataFrame(index = input_trainval.index, columns = multi_index_tau_folds) \n",
    "\n",
    "# for tau in PI_percentiles:\n",
    "#     for fold_idx in np.arange(num_cv_folds):\n",
    "#         # Get path to trained model .pb file\n",
    "#         path_to_curr_model = os.path.join(dir_str.models_dir, \"tau_{:.1%}\".format(tau), \"fold_#{}\".format(fold_idx))\n",
    "        \n",
    "#         # Load model and save in dict\n",
    "#         rescue_model_set[(tau, CV_fold)] = tf.keras.models.load_model(path_to_curr_model, compile = False)\n",
    "        \n",
    "#         # Deploy model on the trainval data and record inference results\n",
    "#         pred_trainval.loc[:, (tau, fold_idx)] = rescue_model_set[(tau,fold_idx)].predict(input_trainval.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Inference\n",
    "After the model is trained, we use it to produce quantile predictions on the entire trainval set. Note that the training history is also transformed into a np array here for easy storage and visualizaiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize container for the inference result for different quantiles/CV folds\n",
    "multi_index_tau_folds = pd.MultiIndex.from_product([PI_percentiles, range(num_cv_folds), output_trainval.columns.values],\n",
    "                                                   names = ['Quantiles','Fold ID', \"Output_Name\"])\n",
    "pred_trainval = pd.DataFrame(index = input_trainval.index, columns = multi_index_tau_folds) \n",
    "\n",
    "# Initialize container for the training loss and metric history\n",
    "num_metrics = len(history[(PI_percentiles[0], 0)].history)\n",
    "training_history = np.ones((len(PI_percentiles), num_cv_folds, max_epochs, num_metrics))*np.nan\n",
    "\n",
    "# looping through all target percentiles and CV folds\n",
    "for i,tau in enumerate(PI_percentiles):\n",
    "    print ('Inferring on quantile of {:.1%}'.format(tau))\n",
    "    for fold_idx in range(num_cv_folds):\n",
    "        # Record training loss and metrics history\n",
    "        num_epochs = len(history[(tau, fold_idx)].history['loss'])\n",
    "        training_history[i,fold_idx,:num_epochs,:] = pd.DataFrame(history[(tau, fold_idx)].history).values\n",
    "        \n",
    "        # Deploy model on the trainval data and record inference results\n",
    "        pred_trainval.loc[:, (tau, fold_idx)] = rescue_model_set[(tau,fold_idx)].predict(input_trainval.values)\n",
    "        \n",
    "#Output inference result and training history to hard drive        \n",
    "pred_trainval.to_pickle(dir_str.pred_trainval_path)\n",
    "np.save(dir_str.training_hist_path, training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Diagnostics and Visualizations\n",
    "This section includes an expanding set of diagnostics and visualization tool to assess the performance of RESCUE model, and check whether the model's behavior adheres to our intuition. As a standalone section, you should be able to skip **section 2-4** and directly start runing **section 5** after **section 1**. First we load in the quantile prediction made in the inference section and the training history from the training section. They should all be on the hard drive already if previously trained to completion and saved. Then, users can optionally read in reserve produced by other methodology for comparison, by providing associated inputs in **section 0.2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in quantile prediction\n",
    "if os.path.exists(dir_str.pred_trainval_path):\n",
    "    pred_trainval = pd.read_pickle(dir_str.pred_trainval_path)\n",
    "else:\n",
    "    print('No quantile prediction found! Run section 4 first!')\n",
    "# load in training history\n",
    "if os.path.exists(dir_str.training_hist_path):\n",
    "    training_hist = np.load(dir_str.training_hist_path)\n",
    "else:\n",
    "    print('No training history found!')\n",
    "    \n",
    "# Read in comparison reserves data, if user provided a path to it\n",
    "if is_plotting_comparative_methods:\n",
    "    comparative_reserves = {}\n",
    "    for comp_name, path in comparative_reserves_path.items():\n",
    "        comparative_reserves[comp_name] = pd.read_csv(path, index_col = 0, parse_dates = True)\n",
    "        # Extract appropriate time-points from historical reserves data\n",
    "        comparative_reserves[comp_name] = comparative_reserves[comp_name].loc[input_trainval.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Numeric Metrics\n",
    "The sections below contains numeric metrics generated for the RESCUE model.\n",
    "### 5.1.1 Metrics matrix\n",
    "For the metrics matrix, it places different prediction interval on different columns and different metrics on the rows. The metrics currently includes:\n",
    "- coverage\n",
    "- requirement\n",
    "- exceeding\n",
    "- closeness\n",
    "- max exceedance\n",
    "- reserve ramp rate\n",
    "- pinball loss\n",
    "\n",
    "The terminology for the metrics largely follows [CAISO FRP](http://www.caiso.com/InitiativeDocuments/AppendixC-QuantileRegressionApproach-FlexibleRampingProductRequirements.pdf) convention. For more information, please refer to the documentation within metrics module for each of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated metrics value for the RESCUE method\n",
    "metrics_value = metrics.compute_metrics_for_all_taus(output_trainval, pred_trainval, val_masks_all_folds, dir_str)\n",
    "\n",
    "# print out net load related metrics\n",
    "for output in metrics_value.columns.levels[1]:\n",
    "    if \"Net_Load\" in output:\n",
    "        print(\"=\"*84,\"\\n\", output, \"\\n\", \"=\"*84)\n",
    "        print(metrics_value.xs(output, axis=1, level = 'Output_Name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Quantile crossing\n",
    "Theoretically, for any type of forecast error, larger quantiles should always have a higher(more positive) numeric value. However, when this is not the case with two quantiles, it is called an instance of quantile crossing. This section characterizes the numbers of occurence for this phenomenom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.n_crossings(pred_trainval).style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Tensorboard\n",
    "Tensorboard is a built-in visualization tool to visualize the losses and metrics during multiple training processes. Use `tensorboard --logdir dir_str.logs_dir` to invoke it in the command line (replace `dir_str.logs_dir` with the folder it's referring to). This is also the reason we did not plot training history for each epoch, as it's done in tensorboard already\n",
    "\n",
    "**What to look for**: Training loss should converge in the last few epochs. Validation loss should not trail behind training loss by too much.\n",
    "\n",
    "## 5.3 Visualization\n",
    "Visualize the forecast bias and uncertainty's change w.r.t to different input features. Check if the visualization corroborate or contradict our intuition. Many contradictions would constitute a red flag for model's fidelity.\n",
    "\n",
    "1. One fold at a time: Since visualizing multiple folds' output at the same time brings visual chaos, we use an example fold from the trained models as the basis for visualization.\n",
    "2. Sign convention: The sign convention is managed such that headroom requirement will always be shown above y = 0 and footroom will always be shown below y = 0, irrespective of whether you are looking at load-based or a generation-based forecast uncertainty. Practically this means that for load, upper quadrants are for actual load>predicted load; and for generation, actual generation < predicted generation. \n",
    "3. Mulitple outputs: Different outputs should all be visuallized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(diagnostics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only visualize for the validation predictions\n",
    "pred_val = metrics.get_validation_preds(pred_trainval, val_masks_all_folds)\n",
    "\n",
    "# Based on the traditional foot room and headroom notion, we should have <0.5 quantiles represent footroom, \n",
    "# and >0.5 quantiles represent headroom. Therfore, we flip the generation side quantile prediction's sign\n",
    "# and quantile postiion to conform to this sign convention\n",
    "load_type_response = response_type_map[response_type_map == \"load\"].index\n",
    "idx = pd.IndexSlice # alias for pandas slicer\n",
    "\n",
    "# switching the headroom and footroom columns for the generation side errors. \n",
    "# I.e. The PI quantiles would be switched to the 1-PI position\n",
    "for PI in PI_percentiles:\n",
    "    if PI<0.5:\n",
    "        sym_PI = round(1-PI, 4) #avoiding numeric issue\n",
    "        transfer_array = pred_val.loc[:,idx[PI, load_type_response]].values\n",
    "        pred_val.loc[:,idx[PI, load_type_response]] = pred_val.loc[:,idx[sym_PI, load_type_response]].values\n",
    "        pred_val.loc[:,idx[sym_PI, load_type_response]] = transfer_array\n",
    "\n",
    "# Flip the sign for the generaion side errors and reserves in addition to switching position\n",
    "pred_val.loc[:, idx[:, load_type_response]] *=-1\n",
    "output_trainval.loc[:, load_type_response] *= -1\n",
    "\n",
    "# This function essentially puts any given plotting function in a loop, \n",
    "# which goes thru all the responses listsed in the label_to_response_map\n",
    "loop_thru_curr_responses = functools.partial(diagnostics.loop_thru_responses, pred_val = pred_val, output_trainval = output_trainval, \n",
    "                                             label_to_response_map = label_to_response_map, plot_dir = dir_str.plots_dir,\n",
    "                                             is_plotting_comparative_methods = is_plotting_comparative_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Forecast uncertainty grouped by input features\n",
    "Visualize the forecast bias and uncertainty's change w.r.t to different input features. Check if the visualization corroborate or contradict our intuition. Many contradictions would constitute a red flag for model's fidelity.\n",
    "\n",
    "**What to expect with net load forecast:** <br>\n",
    "1. **Forecast uncertainty v.s. Solar production**: Forecast uncertainty should be the highest for medium solar production, and low during both no solar and full solar periods.\n",
    "2. **Forecast uncertainty v.s. Wind production**:  Wind should have less effect on forecast uncertainty than solar. Increases with higher wind production\n",
    "3. **Forecast uncertainty v.s. Load**: Forecast uncertainty should increase with higher load.\n",
    "4. **Forecast uncertainty v.s. Hours**: Sunset and sunrise hours are associated with high amount of uncertainty. No solar night time hours should have low uncertainty.\n",
    "5. **Forecast uncertainty v.s. Season**: Highly dependent on local climate type. For mediterranean climate, expect little to no change, with a slight increase in winter. For continental climate, expect higher uncertainty in summer due to precipitation. For desert climate, expect smaller uncertainty year round, while expecting higher uncertainty year round for oceanic and tropical climates. For monsoon dominated climate, expect higher uncertainty in monsoon season (usually summer).\n",
    "6. **Forecast uncertainty v.s. Date of installation**: Highly dependent on modeled BA. If the BA experience significant renewable growth, expect increasing uncertainty with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the change in forecast uncertainty with respect to Solar, wind and load\n",
    "for feature, feature_label in label_to_feature_map.items():\n",
    "    # if looking at Hour and Month, binning is derived from index\n",
    "    if feature_label == \"Hour\":\n",
    "        feature_discretized = pred_val.index.hour\n",
    "    elif feature_label == 'Month':\n",
    "        feature_discretized = pred_val.index.month\n",
    "    else:\n",
    "        # else the label is derived from dicretizing a continuous input\n",
    "        feature_discretized = diagnostics.discretize_input(input_trainval[feature])\n",
    "        if feature_label == 'Date of Observation':\n",
    "            # for 'Date of Observation', we add the starting date so it shows up as a historical data\n",
    "            feature_discretized = (pd.Timestamp(input_trainval.index[0].date()) \n",
    "                                   + feature_discretized.astype('int')*pd.Timedelta('1D'))\n",
    "    \n",
    "    comparative_reserve_groupedby_input = {n:v.groupby(feature_discretized.values).mean() \n",
    "                                           for (n,v) in comparative_reserves.items()}\n",
    "    \n",
    "    \n",
    "    plot_uncertainty_groupedby_feature = functools.partial(diagnostics.plot_uncertainty_groupedby_feature, \n",
    "                                                           input_var_discretized = feature_discretized, \n",
    "                                                           input_var_name = feature_label)\n",
    "    title_fn = lambda response: '{}_vs_{}.svg'.format(response, feature_label)\n",
    "    fig, ax = loop_thru_curr_responses(plot_uncertainty_groupedby_feature, title_fn,\n",
    "                                       comparative_reserves = comparative_reserve_groupedby_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.2 Monthly comparison of the coincident quantile between Predictions and Truth\n",
    "The goal of this section is to compare model prediction with historical error quantile. The goal is straightforward but the method is nothing but, since there is no direct way we can observe the historical error quantile.\n",
    "\n",
    "Over a period of time tho, we can use the quantiles of all errors occured in this period to approximate the true quantile of the historical error. For a quantile, we picked the hour that the error falls on this quantile, and plot the coincident model prediction of the same quantile. The authors decided that the most interesting form of this plot, is to aggregate all errors of the same month and hour, and plot out the hourly coincident error and prediction. Reserves calculated from other methods can be overlaid on top for comparison with model predicted reserves. See explainition under **section 5.3** for sign convention.\n",
    "\n",
    "**What to expect from a well trained model:**\n",
    "1. True forecast errors are generally comprised within the extent of model predictions\n",
    "2. Model's predictions doesn't fall well beyond true forecast errors all the time - which would be an indication that the model is being too conservative and thus wasteful of reserves. We're aiming to achieve target coverage while minimizing amount of reserves held to do so\n",
    "3. Median quantile of true forecast errors matches well with median quantile of model predictions of forecast errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparative_reserves_groupedby_hour = {n:v.groupby(v.index.hour).mean() \n",
    "                                      for (n,v) in comparative_reserves.items()}\n",
    "\n",
    "\n",
    "plot_coincident_quantile_comp = functools.partial(diagnostics.plot_coincident_quantile_comp, \n",
    "                                                  quantiles_list= coincident_comp_quantiles)\n",
    "title_fn = lambda response:\"coincident_pred_hist_comparison_{}.svg\".format(response)\n",
    "fig, ax = loop_thru_curr_responses(plot_coincident_quantile_comp, title_fn,\n",
    "                                   comparative_reserves = comparative_reserves_groupedby_hour)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 Example Time series\n",
    "Visualize the quantile forecast for a few example days.\n",
    "\n",
    "\n",
    "**What to look for**:\n",
    "1. **Smoothness**: Is the reserve interval band changing continuously?\n",
    "2. **Coverage**: How good is the reserve interval band covering the true forecast error? The more the better.\n",
    "3. **Exceeding**: What kind of condition seems to lead to exceedance? Hopefully it should be extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each of the time series range to be plotted\n",
    "for ts_range in ts_ranges:\n",
    "    comparative_reserves_TS = {n:v.loc[ts_range] \n",
    "                               for (n,v) in comparative_reserves.items()}\n",
    "    \n",
    "    \n",
    "    plot_example_ts = functools.partial(diagnostics.plot_example_ts, ts_range = ts_range)\n",
    "    title_fn = lambda response:\"{}_on_{}.svg\".format(response, ts_range)\n",
    "    loop_thru_curr_responses(plot_example_ts, title_fn, comparative_reserves = comparative_reserves_TS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.4 Model performance in different CV folds\n",
    "Visualizing model performance for all the cross validation folds across training and validation sets. An important tool in assessing the generalizability of the model.\n",
    "\n",
    "**What to look for:**\n",
    "1. Distance between training and validation. Smaller distance signifies a small amount of expected performance drop betweeen seen and unseen data, hence better generalizability.\n",
    "2. Spread among different validation folds: The smaller the spread, the more invariant the model is to its data, and hence better generalizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, _ =  diagnostics.plot_compare_train_val(training_hist, PI_percentiles,      \n",
    "                                             metrics_to_idx_map, metrics_to_compare, x_jitter =x_jitter)\n",
    "\n",
    "fig.savefig(os.path.join(dir_str.plots_dir, 'Train_Val_Comparison.png'), dpi=default_dpi, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3.5. Model comparison in Pareto efficiency framework\n",
    "\n",
    "Looking at validation-set only predictions for each CV fold, compares:\n",
    "\n",
    "- Requirement\n",
    "- Pinball Loss\n",
    "- Standard Deviation of Pinball Loss\n",
    "- Deviation of Coverage from Target Coverage\n",
    "\n",
    "for all available target quantiles or $\\tau$-levels for net load forecast error prediction.\n",
    "\n",
    "\n",
    "## Background on model comparison and Pareto efficiency framework\n",
    "\n",
    "One often wishes to robustly compare the performances of two or more model configurations to determine whether one model configuration is \"better,\" in a technical sense. There are many performance metrics that one may choose from to compare probabilistic forecasting models. In this notebook, we have defined several: \"coverage\", \"requirement\", \"closeness\", \"exceedance\", and \"pinball loss\". Each may be interpreted to describe a different aspect of a model's performance. For example, \"coverage\" describes a model's ability to make conservative probabilistic forecasts and thus exceed or \"cover\" the forecast error of a particular period at a desired rate. \"Requirement\" describes the average size of forecasts from a model seeking to achieve a particular coverage probability. \"Pinball loss\" may be interpreted as describing a quantile regression model's performance relative to a theoretically optimal quantile regression model, which will achieve minimal pinball loss. There are also others which we derive from these metrics, such as the \"standard deviation of pinball loss across cross-validation folds\". This metric is interpreted as describing the stability of a model's performance given different sets of training and testing data.\n",
    "\n",
    "It is often the case that desirable model characteristics, and the performance metrics which seek to describe them, behave antagonistically with one another. E.g. models that achieve high coverage probability also tend to suffer from high average requirement, as the simplest way to improve coverage probability is to increase the average requirement. Only a model that is exceptionally capable of accurately predicting the conditional quantiles of the response variable will achieve *both* a high coverage probability and a low average requirement. To account for such antagonism between desirable model characteristics, we must often compare multiple performance metrics at once.\n",
    "\n",
    "Put another way, notions of \"betterness\" or \"improvement\" in model performance are often best described using *two or more* performance metrics simultaneously. Continuing the above example, a model with high coverage probability yet high average requirement is not necessarily \"better\" than one with lower coverage probability and lower average requirement. However, a model with higher coverage probability and lower average requirement certainly is better. Thus, in order to compare models, we must simultaneously compare two or more performance metrics. This is the basis of the \"Pareto efficiency framework.\"\n",
    "\n",
    "In this framework, we select and compare metrics that describe distinct aspects of model performance. This comparison is done using a so-called \"Pareto plot\", in which each axis of the plot represents a different performance metric, and models are represented as points corresponding to their measured performance. For each metric, an improvement direction is identified. E.g. we desire a model that minimizes requirement and minimizes fluctuations in coverage probability across different cross-validation folds. Thus, if we plot models as points in the space of \"requirement\" versus \"coverage root-mean-square-error\", the best models are those that lie closest to the origin.\n",
    "\n",
    "In the particular case of this project, we wish to compare different \"model configurations\" that are iterated over multiple target quantiles or $\\tau$-levels. Therefore each model configuration corresponds to multiple individual models, each predicting at a different target quantile. We have taken the approach of plotting all model instances corresponding to a particular model configuration simultaneously in a Pareto plot. While this approach may obfuscate attempts to determine whether one model configuration is superior to another (as each model configuration is evaluated at several different points), it still allows one to make useful qualitative comparisons between models. E.g. one may observe whether one model configuration is better on average than another model configuration, or even whether it is better over particular regimes of target quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating metrics for  rescue_v1_1_multi_objective\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\charles.gulian\\\\PycharmProjects\\\\RESCUE\\\\data\\\\rescue_v1_1_multi_objective\\\\output_trainval.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e1501b6feb62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiagnostics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_multiple_model_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels_to_compare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_for_pareto_comp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\PycharmProjects\\RESCUE\\code\\diagnostics.py\u001b[0m in \u001b[0;36mget_multiple_model_metrics\u001b[1;34m(models_to_compare, output_for_pareto_comp)\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[1;31m# Read in predictions, targets, and validation masks for the current model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[0mpred_trainval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred_trainval_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m         \u001b[0moutput_trainval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_trainval_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m         \u001b[0mnum_cv_folds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_trainval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m         val_masks_all_folds = cross_val.get_CV_masks(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\e3rescue\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"infer\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# 1) try standard library Pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\e3rescue\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\charles.gulian\\\\PycharmProjects\\\\RESCUE\\\\data\\\\rescue_v1_1_multi_objective\\\\output_trainval.pkl'"
     ]
    }
   ],
   "source": [
    "model_metrics = diagnostics.get_multiple_model_metrics(models_to_compare, output_for_pareto_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requirement v.s. Coverage RMSE**\n",
    "\n",
    "Comparison of \"requirement,\" which we desire to be close to 0, versus \"coverage root-mean-squared-error\" (taken over cross-validation folds) which we desire to minimize.\n",
    "\n",
    "Interpretation: Pareto-optimal models in this comparison will simultaneously minimize the average reserve requirement (a measure of efficiency) while minimizing deviations of the achieved coverage probability from the target quantiles (a measure of performance stability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = diagnostics.plot_pareto_coverage_rmse_vs_req(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pinball loss v.s. Pinball loss standard deviation**\n",
    "\n",
    "Comparison of \"pinball loss,\" which we desire to minimize, versus \"pinball loss standard deviation\" (taken over cross-validation folds) which we also desire to minimize.\n",
    "\n",
    "Interpretation: Pareto-optimal models in this comparison will simultaneously minimize pinball loss (a general performance metric for quantile regression models, which is theoretica) while also minimizing fluctuations in the observed pinball loss (a measure of performance stability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = diagnostics.plot_pareto_pinball_loss_vs_loss_std(model_metrics)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "advanced.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
